---
title: "Geocomputation"
author: "Kaz Sakamoto"
output: html_document
---
<style>
@import url('https://fonts.googleapis.com/css?family=Noto+Sans|VT323');
body{
  font-family: 'Noto Sans', sans-serif;
  font-size: 12px;
  line-height: 24px;
}

h1,h2,h3,h4 {
  font-family: 'VT323', monospace;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, cache = TRUE, message = FALSE)
```

## Spatial Libraries

```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(sf)
library(raster)
library(tmap)

## here we are going to find the root_file 
root <- rprojroot::find_rstudio_root_file()
## we are going to set the data file path here
dataDir <- file.path(root, 'Data')
```

## Data Models

### Vector

### Raster

## Simple Features
There are seventeen geometry types supported by the Open Geospatial Consoritum(OGC),
but there are seven that are most commonly used. As you can see these are all vector
formats, and `sf` does not handle raster data. 
![](https://geocompr.robinlovelace.net/figures/sf-classes.png)

The `sf` package relies on the `rgdal` package for reading and writing spatial data and
`rgeos` for spatial operations. 
```{r}
library(spData)
names(world)
```

```{r}
plot(world)
```

## Coordinate Reference 

The coordinate reference system is crucial since it ties the vector and raster data types to a location on the earth (or other bodies). To get this reference system, they rely on geodetic datums which are reference points located all around the earth. The most common datums we use ar the WGS 84 and NAD 83. Because the earth is not completely smooth nor sphereical, the common datums that we use approximate an ellipsoid, which is due to the fact the earth's rotation and effects of gravity flatten the poles while the equator slightly bulges. Many localities have specific datusm or elliposoid models to use and in the US the NAD 83 is very common. This data will me located in the `ellps` parameter of the `PROJ CRS` library. 

Here are the important features when modeling the earth.
- Actual Earth's surface:  has mountains and oceans.
- Geoid: the shape that the ocean surface would take under the influence of the gravity and rotation of Earth alone, if other influences such as winds and tides were absent.
- ellipsoid: The approximation of the Earth's surface into a more regular shape. 
![](https://www.aalto.fi/sites/g/files/flghsv161/files/styles/medium/public/2018-06/surfacecomparison.jpg?itok=Zq-ObTTp)

Also when you are working with multiple files with different CRSs you will have to transform them to a common CRS or they will not align. You can think of this as trying to do any calculation with data with different units. 

### Geographic Coordinate Systems

Geographic coordinate systems use angular measurements to locate a place on the surface of the earth. These measures are called Latitude and Longitude, they are a measure from the center of earth to the location on the surface. Longitude is the E-W distance from the Prime Meridian plane, while latitude is the N-S distance from the equatorial plane. 

One important aspect to highlight is that these are *angular* measurements. This means that it is not a distance like meters or miles and there fore a distance calculation is not straight forward since we are still talk about a curved surface of the earth and its location.

### Projected Coordinate Systems

Projected coordinate systems are based on Cartesian coordinates if you recall making graphs in math classes with x and y locaitons. Unlike the angular measurements of the geographic coordinate systems, these are *linear* measusrements like meters and feet. Projected coordinate systems are based on geographic coordinate systems but take the 3D object of the earth and project it into 2D. 

This projection will always cause some sort of distortion when you go from a 3D object to a 2D. The classic example is trying to flatten a orange peel, you can't do that with out tearing the skin, flattening and stretching it out. 

![](https://learngis.org/Images/smashed_orange01.jpg)

There are certain properties that we want to accurately capture when we are working with mapping, these are area, distance, direction, and shape. A projected coordinate can only preserve one or two properties, because of this the map maker must think about the trade-offs being made when projecting the earth on to a 2D map. This is apparent when you think about the debate betwen the "Euro-centric" Mercator projection that we have all become accostumed to compared with the Peters Projection that accurately displays areas. 
![](https://external-preview.redd.it/6Vagn3BQ1Mwh4kN2-uldK7X8UM9_QAvYJCN2CCvoLWs.png?auto=webp&s=ca818d0e1049c02e6c0e82bf9223250822f30565)

The last thing to cover is the three main groups of projections types which are conic, cylindrical and planar or azimuths. As you can probably infer by their names these are the shapes that are being used to capture the projection, if you think of earth with a light bulb in the middle and place these objects on the surface of the earth, you can see that the earths surface would be projected on the shape. Distortions are minimized where the projectino shape is tangent with the earths surface and greater the further you get from it. These shapes either can have a single line of tangency or two lines of tangency also.

![](https://www.acsu.buffalo.edu/~dbertuca/maps/cat/cylindrical_final.gif)

## Working with CRSs

You can work with the CRS of geographic data with `epsg` codes or `proj4string` defintions. `epsg` codes are shorter and might be easier to remember while the `proj4string` will allow you more flexibility when dictating projections type, datum, and ellpsoid parameters. `epsg` only referes to one specific CRS which does not allow you to change different parameters. 


```{r}

nybb <- read_sf(file.path(dataDir, "gis/nybb"))
st_crs(nybb)
```

```{r}
centralPark = data.frame(lon = -73.9691305, lat = 40.7764627) %>% 
  st_as_sf(coords = c("lon", "lat"))
st_is_longlat(centralPark)
```

```{r}
centralPark_geo = st_set_crs(centralPark, 4326)
st_is_longlat(centralPark_geo)
```

Here you can see that only the second dataset created a warning. We are trying to run a distance based
function, creating a buffer around a data with latitude and longitude. 
```{r}
centralPark_buff_no_crs = st_buffer(centralPark, dist = 1)
cebtralPark_buff = st_buffer(centralPark_geo, dist = 1)
```

```{r}
tm_shape(centralPark) + tm_symbols() + 
    tm_shape(nybb) + tm_polygons()
```


### Areal Exercises

- [Equal Area](https://epsg.io/102008)
- [Equal Distance](http://epsg.io/102005)
- [Small Shapes Maintained](http://epsg.io/102004)
- [State Plane](https://epsg.io/2263)

```{r}
Area_Albers <- 102008
Area_Equidistant <- 102005
Area_Lambert <- 102004
Area_NYStatePlane <- 2263
projections <- setNames(c(Area_Albers, Area_Lambert,Area_Equidistant, Area_NYStatePlane),
         c("Area_Albers", "Area_Lambert","Area_Equidistant", "Area_NYStatePlane")) %>% 
    as.list()
```

1 square meter  = 10.7639 square feet.

```{r}
massReproject <- function(shape, projection){
    # we have to reporject and recalulate the area
    st_transform(shape, projection) %>% st_area()
}

areaCalc <- purrr::map_df(projections, ~massReproject(nybb, .x)) %>% 
    mutate_at(vars(Area_Albers, Area_Equidistant, Area_Lambert), function(x) x*10.7639) %>% 
    mutate(BoroName = nybb$BoroName) %>% 
    mutate(Albers_Diff = Area_Albers-Area_Albers,
           Lambert_Diff = Area_Albers- Area_Lambert,
           Equidistant_Diff = Area_Albers-Area_Equidistant,
           NYStatePlane_Diff = Area_Albers-Area_NYStatePlane)

areaCalc %>% dplyr::select(5:9) %>% 
    tidyr::gather(projection, difference, -BoroName) %>% 
    ggplot(aes(projection, difference, fill = difference)) +
    geom_col() + 
    coord_flip()+ 
    facet_wrap(~BoroName, scales = "free") + 
    theme_minimal() 
```

## Attribute data operations

the `sf` library is great because it extends the data.frame and adds geographic features to it. Much like the tidy data
paradigm, each row is still an observations and each column is a feature. The main difference between a normal data.frame 
and an sf object is that there is another column `geometry` baked in which can contain a multitude of geographic types
like points, lines and polygons. 

You will see that a lot of methods that you are used to working with in data.frames will apply to sf objects. Like `rbind`,
`$<-`, and `cbind`.
```{r}
methods(class = "sf")
```

If we ever wanted to go back to a normal data.frame object it's very simple
```{r}
st_drop_geometry(nybb) %>% class()
```

### Subsetting

```{r}
nybb[1,]
nybb[,c(2,4)]
```
```{r}
nybb %>% slice(1:2)
nybb %>% dplyr::select(2)
```


```{r}
nybb %>% filter(Shape_Area >= 1937566944)
```


## Joining data

```{r}
library(rvest)
demographics <- read_html("https://en.wikipedia.org/wiki/Demographics_of_New_York_City") %>% 
    html_nodes(xpath = '//*[@id="mw-content-text"]/div/table[1]') %>% 
    html_table(header = FALSE) %>% "[["(1) %>% slice(4:8) %>% 
    mutate(X1 = c("Bronx", "Brooklyn", "Manhattan", "Queens", "Staten Island")) %>% 
    dplyr::select(c(1,3:5))

names(demographics) <- c("BoroName", "population",
                         "GDP", "GDPperCapita")
```

```{r}
nybb_demo <- nybb %>% left_join(demographics) %>% 
    mutate_at(vars(population, GDP, GDPperCapita), function(x) stringr::str_remove_all(x, ",") %>% as.numeric())

plot(nybb_demo["population"])

```

## Spatial Operations

### intersects

```{r}
jul14 <- read_csv("https://raw.githubusercontent.com/fivethirtyeight/uber-tlc-foil-response/master/uber-trip-data/uber-raw-data-jul14.csv")

uberJuly <- st_as_sf(jul14, coords = c("Lon", "Lat"),crs = 4326)
set.seed(1234)
uberJuly <- st_transform(uberJuly, 2263) %>% sample_frac(0.2)
set.seed(1234)
uberSamp <- uberJuly %>% sample_n(100)
```
```{r}
tm_shape(nybb) + tm_polygons()+
    tm_shape(uberSamp) + tm_symbols(size = .2, col = "white") 
```
```{r}
st_crs(nybb) <- 2263
st_intersects(uberSamp, nybb)
st_intersects(uberSamp, nybb, sparse = FALSE)
```

### disjoint
```{r}
st_disjoint(uberSamp, nybb)
```

### within

```{r}
st_within(uberSamp, nybb)
```

### within distance

```{r}
sel = st_is_within_distance(uberSamp, nybb, dist = 100)
lengths(sel) > 0
```

### Selecting with Intersection
`sgbp`  is a sparse geometry binary predicate
```{r}
sel_nycUber = st_intersects(x = uberJuly, y = nybb)
class(sel_nycUber)
#> [1] "sgbp"
sel_logical = lengths(sel_nycUber) > 0
nycUber = uberJuly[sel_logical, ]

set.seed(1234)
nycUberSamp <- nycUber %>% sample_n(100)

tm_shape(nybb) + tm_polygons()+
    tm_shape(nycUberSamp) + tm_symbols(size = .2, col = "white") 
```

### Spatial Joining
```{r}
st_crs(nybb_demo) <- 2263
uberJoin <- st_join(nycUber, nybb_demo)
```

```{r}
tm_shape(nybb) + tm_polygons(col = "white")+
    tm_shape(uberJoin) + tm_symbols(size = .2, col = "BoroName", 
                                    alpha = 0.4) 
```

### Spatial Data aggregation
```{r}
uberJuly %>% 
    mutate(x = rnorm(n = 100 )) %>% aggregate(by = nybb, FUN = mean)
```

### Distances

When we were looking at topological realationships, the results were only binary, either they
intersect or does not, etc...

```{r}
nybb_centroid = st_centroid(nybb)

st_distance(uberJuly[1,], nybb_centroid)
```

```{r}
st_distance(uberJuly[1:10,], nybb_centroid)

```

```{r}
plot(st_geometry(nybb_centroid))
plot(st_geometry(uberJuly)[1:10], add = TRUE, col = "red")
```

## Geometric Operations

### Simplification

```{r}
nyc_simp <- st_simplify(nybb, dTolerance = 500)
plot(nyc_simp["BoroName"])
plot(nybb["BoroName"])
```

```{r}
object.size(nybb)
object.size(nyc_simp)
```

```{r}
nyc_ms = rmapshaper::ms_simplify(nybb, keep = 0.50,
                                          keep_shapes = TRUE)
```

### Centroids
```{r}
nybb_centroid = st_centroid(nybb)

tm_shape(nybb) + tm_polygons()+
    tm_shape(nybb_centroid) + tm_symbols() 
```

```{r}
subway <- read_sf(file.path(dataDir, "gis", "SubwayLines")) %>% 
    st_transform(2263)
tm_shape(nybb) + tm_polygons(col ="white")+
tm_shape(subway) + tm_lines()
```

```{r}

subwayRT <- subway %>% pull(rt_symbol) %>% unique()

splitSubway <- subway %>% tidyr::nest(-rt_symbol) %>% 
    dplyr::mutate(combine = purrr::map(data, st_combine))

subwayLines <- splitSubway$combine %>% purrr::reduce(c) %>%
    st_sf() %>% mutate(lines = subwayRT)

subwayCentroids <- sf::st_centroid(subwayLines)
subwaysurface <- st_point_on_surface(subwayLines)
```

```{r}
tm_shape(nybb %>% filter(BoroName != "Staten Island")) + tm_polygons(col = "white")+
tm_shape(subwayLines) + tm_lines(col = "lines") +
    tm_shape(subwayCentroids) + tm_symbols(col = "lines", size =0.5, shape = 3)+
    tm_shape(subwaysurface) + tm_symbols(col = "lines", size = 0.5, shape = 5)
```

### Buffers

```{r}
subwayQuarterBuffer = st_buffer(subwayLines, dist = 1320)
subwayQuarterBuffer
```

```{r}
tm_shape(nybb %>% filter(BoroName != "Staten Island")) + tm_polygons(col = "white") +
tm_shape(subwayQuarterBuffer) + tm_polygons(col = "lines", alpha = 0.2) 
```

### Clipping

![](https://geocompr.robinlovelace.net/figures/venn-clip-1.png)

```{r}
sel_subwayUber = st_intersects(x = uberJuly, y = subwayQuarterBuffer)
sel_subwayUber = lengths(sel_subwayUber) > 0
subwayUber = uberJuly[sel_subwayUber, ]

tm_shape(nybb) + tm_polygons()+
    tm_shape(subwayUber) + tm_symbols(size = .2, col = "white") 
```

Inverse choice
```{r}
sel_subwayUber = st_intersects(x = uberJuly, y = subwayQuarterBuffer)
sel_subwayUber = lengths(sel_subwayUber) < 1
subwayUber = uberJuly[sel_subwayUber, ]

tm_shape(nybb) + tm_polygons()+
    tm_shape(subwayUber) + tm_symbols(size = .2, col = "white") 
```



### Rasterize
A RasterLayer can easily be created from scratch using the function raster.
The default settings will create a global raster data structure with a longitude/latitude coordinate reference system and 1 by 1 degree cells. You can
change these settings by providing additional arguments such as xmin, nrow,
ncol, and/or crs, to the function. You can also change these parameters after
creating the object. If you set the projection, this is only to properly define it,
not to change it. To transform a RasterLayer to another coordinate reference
system (projection) you can use the function projectRaster

let's rasterize the uber data
```{r}
uberSP <- as(nycUber %>% dplyr::select(-`Date/Time`,-Base), "Spatial")
uberSP = spTransform(uberSP, CRS("+init=epsg:2263"))
rast <- raster(crs =  CRS("+init=epsg:2263"))

extent(rast) <- extent(uberSP)
ncol(rast) <- 50
nrow(rast) <- 50

rast2 <- rasterize(uberSP, rast, fun=function(x,...)length(x))

plot(rast2)

raster::contour(rast2, add = TRUE)
raster::filledContour(rast2)

```


```{r}
uberSamp$density <- raster::extract(rast2, uberSamp)

tm_shape(nybb) + tm_polygons(col = "white")+
    tm_shape(uberSamp) + tm_symbols(size = .2, col = "density") 
```


## Kerenel Density
```{r}
library(spatstat)
library(maptools)
uberSampSP  <- as(uberSamp, "Spatial")
uberSampPPP <- as(uberSampSP, "ppp") 
```

```{r}
kernelUber <- stats::density(uberSampPPP)
plot(kernelUber, main=NULL, las=1)
raster::contour(kernelUber, add = TRUE)
```

```{r}

tm_shape(raster(kernelUber)) + tm_raster()+
tm_shape(nybb) + tm_borders()
```

## First and Second Order Effects

Density based measurements such as kernel density estimations look at the 1st order property of the underlying process. Distance based measurements such as ANN and K-functions focus on the 2nd order property of the underlying process.

It’s important to note that it is seldom feasible to separate out the two effects when analyzing point patterns, thus the importance of relying on a priori knowledge of the phenomena being investigated before drawing any conclusions from the analyses results.

## Hypothesis Testing

Popular spatial analysis techniques compare observed point patterns to ones generated by an independent random process (IRP) also called complete spatial randomness (CSR). CSR/IRP satisfy two conditions:

Any event has equal probability of being in any location, a 1st order effect.

Average Nearest Neighbors. 

$$ANN_{ratio}=\dfrac{ANN}{ANN_{expected}}$$

Average Nearest Neighbor Expected
$$ANN_{Expected}=\dfrac{0.5}{\sqrt{n/A}} $$


The parameter k can take on any order neighbor (up to n-1 where n is the total number of points).

The average nearest neighbor function can be expended to generate an ANN vs neighbor order plot. In the following example, we’ll plot ANN as a function of neighbor order for the first 100 closest neighbors:

```{r}
annUber <- mean(nndist(uberSampPPP, k=1))
mean(nndist(uberSampPPP, k=2))
ANN <- 1:(uberSampPPP$n-1) %>% as.list() %>% purrr::map_dbl(function(x)mean(nndist(uberSampPPP, k = x)))
plot(ANN ~eval(1:(uberSampPPP$n-1)), type = "b",main = "Average Nearest Neighbor")
```
The bottom axis shows the neighbor order number and the left axis shows the average distance in feet.

Do you see a problem here? Could different shapes encompassing the same point pattern have the same surface area? If so, shouldn’t the shape of our study area be a parameter in our ANN analysis? Unfortunately, ArcGIS’ ANN tool cannot take into account the shape of the study area. An alternative work flow is outlined in the next section.



The location of one event is independent of the location of another event, a 2nd order effect.

Note that we now have two competing hypotheses: a CSR/IRP process and median income distribution. Both cannot be rejected. This serves as a reminder that a hypothesis test cannot tell us if a particular process is the process involved in the generation of our observed point pattern; instead, it tells us that the hypothesis is one of many plausible processes.

It’s important to remember that the ANN tool is a distance based approach to point pattern analysis. Even though we are randomly generating points following some underlying probability distribution map we are still concerning ourselves with the repulsive/attractive forces that might dictate the placement of Walmarts relative to one another–i.e. we are not addressing the question “can some underlying process explain the X and Y placement of the stores”.


Remember the avereage nearest neighbor for uber is 2497 feet
```{r}
annUber
```

Now we'll vreat eh simuation using Monte Carlo Methods.
```{r}
n     <- 599L               # Number of simulations
ann.r <- vector(length = n) # Create an empty object to be used to store simulated ANN values
for (i in 1:n){
  rand.p   <- rpoint(n=uberSampPPP$n, win= as.owin(sf::st_bbox(nybb)))  # Generate random point locations
  ann.r[i] <- mean(nndist(rand.p, k=1))  # Tally the ANN values
}
```

```{r}
Window(rand.p) <- as.owin(sf::st_bbox(nybb))
plot(rand.p, pch=16, main=NULL, cols=rgb(0,0,0,0.5))
```
```{r}
tibble(ANN = ann.r) %>% ggplot(aes(x = ANN)) + 
    geom_histogram(fill = "green", col = "gray") +theme_minimal()
```

```{r}
annUber
tibble(ANN = ann.r) %>% ggplot(aes(x = ANN)) + 
    geom_histogram(fill = "green", col = "gray") +
    geom_vline(xintercept = annUber,linetype = "dashed") + theme_minimal()
```

A (pseudo) p-value can be extracted from a Monte Carlo simulation. We’ll work off of the last simulation. First, we need to find the number of simulated ANN values greater than our observed ANN value.

```{r}
N.greater <- sum(ann.r > annUber)
p <- min(N.greater + 1, n + 1 - N.greater) / (n +1)
p
```

In our working example, you’ll note that or simulated ANN value was nowhere near the range of ANN values computed under the null yet we don’t have a p-value of zero. This is by design since the strength of our estimated p will be proportional to the number of simulations–this reflects the chance that given an infinite number of simulations at least one realization of a point pattern could produce an ANN value more extreme than ours.

## Spatial Autocorrelation

from Manny Gimond's [github](https://mgimond.github.io/Spatial/spatial-autocorrelation.html)

![](https://mgimond.github.io/Spatial/img/Random_maps.png)

```{r}
nyct <- read_sf(file.path(dataDir, "gis/nyct2010"))
nycACS <- read_csv(file.path(dataDir, "nycACS2017.csv"))
nycACS %<>% dplyr::select(Geo_COUNTY, Geo_TRACT, SE_A14006_001) %>% 
    dplyr::rename(BoroCode = Geo_COUNTY,
                  CT2010 =Geo_TRACT,
           MedIncome = SE_A14006_001) %>% 
    dplyr::mutate(BoroCode = case_when(
        BoroCode == "047" ~"3",
        BoroCode == "061" ~"1",
        BoroCode == "081" ~"4",
        BoroCode == "085" ~"5",
        BoroCode == "005" ~ "2"
        
    ))
st_crs(nyct)
nyct = st_set_crs(nyct, 2263)

nyct <- nyct %>% left_join(nycACS, by = c("BoroCode" = "BoroCode", 
                                          "CT2010" = "CT2010")) %>% 
    mutate(MedIncome = tidyr::replace_na(MedIncome, 0 ))
```

Defining Neighboring polygons 

for creating neighbors from sf [article](https://cran.r-project.org/web/packages/spdep/vignettes/nb_sf.html)

![](http://www.lpc.uottawa.ca/publications/moransi/image2.gif)

```{r}
library(spdep)
nyctSP  <- as(nyct, "Spatial")


nb <- poly2nb(nyctSP, queen=TRUE)
c(612, 806, 1623)
nb <- subset(nb,
  !(1:length(nb) %in% c(612, 806, 1623)))
nb[[1]]
```

```{r}
nyctSP$CT2010[1]
nyctSP$CT2010[nb[[1]]]
```

```{r}
par(bg = 'black')
plot(nb, coordinates(nyctSP[-c(612, 806, 1623),]), points=FALSE, lwd = 0.5, add = TRUE, col = "#DC16C3")
```

Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight `(style="W")`. This is accomplished by assigning the fraction $1/(\text{# of neighbors})$to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the `style="W"` option for simplicity’s sake but note that other more robust options are available, notably `style="B"`.
```{r}
lw <- nb2listw(nb, style="W", zero.policy=TRUE)
```
To see the weight of the first polygon’s four neighbors type:
```{r}
lw$weights[[1]]
```
each neighbor gets a fifth of the weight.


Finally, we’ll compute the average neighbor income value for each polygon. These values are often referred to as spatially lagged values.

```{r}
Inc.lag <- lag.listw(lw, nyct$MedIncome)
```

## Moran's I
```{r}
moran.test(nyctSP$MedIncome,lw)
```

```{r}
MC<- moran.mc(nyctSP$MedIncome, lw, nsim=599)
MC
```

```{r}
# Plot the distribution (note that this is a density plot instead of a histogram)
plot(MC, main="", las=1)
```

## Moran's I as function fo distance

```{r}
coo <- coordinates(nyctSP)
```
```{r}
S.dist  <-  dnearneigh(coo, 0, 7920)  
c(612, 1677)
S.dist <- subset(S.dist,
  !(1:length(S.dist) %in% c(612, 1677)))
S.dist
```

```{r}
lwD <- nb2listw(S.dist, style="W",zero.policy=T) 
MI  <-  moran.mc(nyct %>% slice(-c(612,1677)) %>% pull(MedIncome), lwD, nsim=599,zero.policy=T) 
MI
```
```{r}
plot(MI, main="", las=1) 

```

### Rayshader

```{r}
elev <- sf::st_read(file.path(dataDir, "gis/Elevationpoints"))
elevs <- elev %>% sample_frac(0.05)
elevsp <- as(elevs["elevation"], "Spatial")
# Create an empty grid where n is the total number of cells
grd              <- as.data.frame(spsample(elevsp, "regular", n=1000))
names(grd)       <- c("X", "Y")
coordinates(grd) <- c("X", "Y")
gridded(grd)     <- TRUE  # Create SpatialPixel object
fullgrid(grd)    <- TRUE  # Create SpatialGrid object

# Add P's projection information to the empty grid
proj4string(grd) <- proj4string(elevsp)

# Interpolate the grid cells using a power value of 2 (idp=2.0)
P.idw <- gstat::idw(elevation~1, elevsp, newdata=grd, idp=2.0)

# Convert to raster object then clip to Texas
r       <- raster(P.idw)
nybblatlong <- st_transform(nybb, 4326)
r.m     <- mask(r, nybblatlong)
plot(r.m)
plot(r)


raster::getValues(r, format = "matrix") %>%
  sphere_shade(texture = "desert")  %>%
  plot_3d(r.m, zscale = 75)
```


```{r}
library(rayshader)
montereybay %>%
  sphere_shade(texture = "desert") %>%
  add_water(detect_water(montereybay), color="desert") %>%
  plot_3d(montereybay,zscale=50)
```


```{r}

```

