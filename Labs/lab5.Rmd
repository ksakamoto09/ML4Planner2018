---
title: "Lab 5"
author: "Kaz Sakamoto"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, cache = TRUE, message = FALSE)
```
```{r}
library(dplyr)
library(magrittr)
library(ggplot2)
library(coefplot)
## here we are going to find the root_file 
root <- rprojroot::find_rstudio_root_file()
## we are going to set the data file path here
dataDir <- file.path(root, 'Data')
```

# Introduction

We will first read data from Jared Lander's website. This data comes from the NYC open data portal
but has already been cleaned. It represents housing sales between 2011-2021 in NYC. 
```{r}
housing <- readr::read_csv("http://www.jaredlander.com/data/housing.csv")
```

let's make the names something more readable:
```{r}
names(housing) <- c("Neighborhood", "Class", "Units", "YearBuilt",
                    "SqFt", "Income", "IncomePerSqFt", "Expense",
                    "ExpensePerSqFt", "NetIncome", "Value", 
                    "ValuePerSqFt", "Boro")
```

# Stats Principles

## Correlation

One measure of the relationship between two Variables is called correlation. The correlation output
is between -1 and 1 with negative meaning an inverse relationship and positive meaning closer relationship.

```{r correlation}
cor(housing$SqFt, housing$Income)
```

we can even see the correlation between multiple variables.

```{r}
cor(housing %>% select(c(Income, SqFt, Units, ValuePerSqFt)))
```

We have already seen this plot but let's plot our new data with `ggpairs`
```{r}
GGally::ggpairs(housing %>% select(c(Income, SqFt, Units, ValuePerSqFt)))
```

## Covariance

Related to correlation, covariance is like a variance between variables. 
It provides a measure of the strength of the correlation between two or more sets of random variates.
Unlike correlation which is divided by the standard deviation the numbers can be outside of -1 and 1. 

```{r covariance}
cov(housing$Income, housing$ValuePerSqFt)
```

Like correlation, we can compare the covariance between multiple variables.
```{r}
cov(housing %>% select(c(Income, SqFt, Units, ValuePerSqFt)))
```


# Simple Linear Regression

Let's take a look at square feet and possible rental income. 
```{r}
housing %>% select(Income, SqFt) %>% head
```

And we can plot the two variables and draw the regression line. 

```{r}
ggplot(housing, aes(x = SqFt, y = Income)) + geom_point(alpha = 1/6, shape = 1) + geom_smooth(method = 'lm') + theme_minimal() +
    ylab("Square Feet") + xlab("Income")
```

let's actually get the model using the lm function.

We are using *formula* notation. On the left of the `~` we are adding our response variable (income),
and on the right we are adding our one predictor variable(SqFt). 

The results show two coefficients. One for the intercept 
```{r}
sqftLM <- lm(Income ~ SqFt, data = housing)

sqftLM

sqftInfo <- summary(sqftLM)
sqftInfo

## MSE
sum(residuals(sqftLM)^2) / df.residual(sqftLM)

##RMSE
sqrt(sum(residuals(sqftLM)^2) / df.residual(sqftLM))

sqftCoef <- as.data.frame(sqftInfo$coefficients[,1:2])
sqftCoef <- within(sqftCoef, {
    Lower <- Estimate - qt(p=.9, df = sqftInfo$df[2]) * `Std. Error`
    Upper <- Estimate + qt(p=.9, df = sqftInfo$df[2]) * `Std. Error`
    coef <- row.names(sqftCoef)
})

ggplot(sqftCoef, aes(x=Estimate, y=coef)) + geom_point() + geom_errorbarh(aes(xmin=Lower, xmax = Upper), height = .3) +
    ggtitle("Rental Comparble Income by Square Feet")

```

# Regression Diagnostics

## R^2
percentage of variance explained
```{r}
#r^2
1 - sum((housing$Income-predict.lm(sqftLM, data.frame(SqFt =housing$SqFt)))^2)/
    sum((housing$Income-mean(housing$Income))^2)
```


## Outliers

If we want to see the outliers the `car` package has some nice tools. The function
`outlierTest` gives the studentized residuals which highlights the extreme
outliers in our data.
$$t_i = r_i (\frac{n-k-2}{n-k-1-r_i^2})^{1/2}$$
In the example below we can see that the red data point greatly effects our linear
regression. 

![](https://newonlinecourses.science.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/09influential/influ_points_02/index.png)

The studentized resideual(`TRES1`) is -19.7990 and absolute values larger than 3 can be called outliers.

![](https://newonlinecourses.science.psu.edu/stat462/sites/onlinecourses.science.psu.edu.stat462/files/09influential/influ_points_01/index.png)


```{r}
car::outlierTest(sqftLM)

car::leveragePlots(sqftLM)
```

## Influential Observations

The `influencePlot` function creates a “bubble” plot of Studentized residuals versus hat values, with the areas of the circles representing the observations proportional to the value Cook's distance. Vertical reference lines are drawn at twice and three times the average hat value, horizontal reference lines at -2, 0, and 2 on the Studentized-residual scale.
```{r}
cutoff <- 4/((nrow(housing)-length(sqftLM$coefficients)-2)) 
plot(sqftLM, which=4, cook.levels=cutoff)
car::influencePlot(sqftLM, id.method="identify", main="Influence Plot", sub="Circle size is proportial to Cook's Distance" )
```

## Non-Normality

Plots empirical quantiles of a variable, or of studentized residuals from a linear model, against theoretical quantiles of a comparison distribution.

```{r}
car::qqPlot(sqftLM, main="QQ Plot")

sresid <- MASS::studres(sqftLM)
data.frame(sresid) %>% ggplot(aes(x =sresid)) + 
    geom_histogram(binwidth = 0.5)  + theme_minimal()
```

## Homoscedasticity

Creates plots for examining the possible dependence of spread on level, or an extension of these plots to the studentized residuals from linear models. We're
interested in the variability of a variable is unequal across the range of values of a second variable that predicts it.

```{r}
# Evaluate homoscedasticity
# non-constant error variance test
car::ncvTest(sqftLM)
# plot studentized residuals vs. fitted values 
car::spreadLevelPlot(sqftLM)
```

## Nonlinerarity

`crPlots` constructs component+residual plots, also called partial-residual plots, for linear and generalized linear models. 

This is a good way to see if the predictors have a linear relationship to the dependent variable. A partial residual plot basically attempts to model the residuals of one predictor against the dependent variable. A component residual plot adds a line indicating where the line of best fit lies. A significant difference between the residual line and the component line indicates that the predictor does not have a linear relationship with the dependent variable

```{r}
car::crPlots(sqftLM)
```

## Non-Independence of Errors

the Durbin-Watston statistic is used to detect the presence of autocorrelation at lag 1 in the residuals (prediction errors) from a regression analysis. 
Remember that we want our predictor variables to be independent. The Durbin-Watson statistic will always have a value between 0 and 4. A value of 2.0 means that there is no autocorrelation detected in the sample. Values from 0 to less than 2 indicate positive autocorrelation and values from from 2 to 4 indicate negative autocorrelation.

Positive autocorrelation means that if the previous value declines, the next value will also decline, while a negative autocorrelation has the opposite effect.
```{r}
# Test for Autocorrelated Errors
car::durbinWatsonTest(sqftLM)
```

## Plotting them together
```{r}
layout(matrix(c(1,2,3,4),2,2))
plot(sqftLM)
```

## coeffcients

## Things to look out for

* residuals have to have mean of zero
* residuals are not autocorrelated - Durban Watson test
* normality or errors
* need more observations than variables.
* no excessive multicollinearity

# Multiple Regression

What happens when you want to have more than two variables and include new predictor variables? You will 
$$ Y = \beta_0 + \beta_1*x_1 + \beta_2*x_2 + ...\text{Error}$$

## Specification Errors

you have to make sure all the relevant variables are in the model or else you will have a systematic error when we want our error to be random.

## Challenges

* visual examination becomes difficult
* multicollinearity
* interactions
* attributing importance to each variable


What happens if you have more than one predictor? you'll have to use multiple regression. 

and we'll have to explore our data. Let's 
```{r}
ggplot(housing, aes(x = ValuePerSqFt)) + geom_histogram()
```

There seems to be a bimodal distribution with peaks at around 80 and 200. We can explore further
by mapping colors with boroughs. We can see that Manhattan and Brooklyn accounted for the main 
peaks exhibiting different distributions. 
```{r}
ggplot(housing, aes(x = ValuePerSqFt, fill = Boro)) + geom_histogram(binwidth = 10) + 
    facet_wrap(~ Boro)
```

How about other variables? what does square footage or units look like?

We can see that for sq footage, there are some with extremely large area. For the
number of units, it follows the same distribution, where most of the sales are in the lower ranges
and a few outliers with large numbers. 

```{r}
ggplot(housing, aes(SqFt)) + geom_histogram()
```

```{r}
ggplot(housing, aes(Units)) + geom_histogram()
```

If we plot SqFt and Units against ValuePerSqFt, we can see if we can trim some outliers.
```{r}
ggplot(housing, aes(SqFt, ValuePerSqFt)) + geom_point()
```

```{r}
ggplot(housing, aes(Units, ValuePerSqFt)) + geom_point()
```

Based on the previous graphs if we can trim units below 1000 and sqft below.

```{r}
ggplot(housing %>% filter(Units < 1000), aes(Units, ValuePerSqFt)) + geom_point()
```

```{r}
housing <- housing %>% filter(Units < 1000)
```

now let's fit a model `condo1` with our response variable being ValuePerSqFt and
predictor variables Units, SqFt, and Boro.
```{r}
housingFact <- housing %>% mutate(Boro = as.factor(Boro))
condo1 <- lm(ValuePerSqFt ~ Units + SqFt + Boro + YearBuilt, data = housingFact)
```

We can view the summary of the model
```{r}
summary(condo1)
```
and view only the coefficients

```{r}
condo1$coefficients
```

with the coefplot library we can view the coefficients of out model
```{r}
library(coefplot)
coefplot(condo1, sort = 'mag')
```

We can now see what difference scale has on our data for Units and SqFt.
```{r}
condo2 <- lm(ValuePerSqFt ~ scale(Units) + scale(SqFt) + Boro, data = housing)
summary(condo2)
```

```{r}
coefplot(condo2, sort = 'mag')
```

```{r}
multiplot(condo1, condo2)
```


## Interaction effects


## Stepwise Variable Selection

The stepwise regression consists of iteratively removing or adding predictors 
in order to find the best subset of variables in the data resulting in the best model.
The criteria for choosing the best model is usually dictated by the lowest prediction errors
through RMSEs. 

There are three ways stepwise selection can be operatinalized:

- **Forward** which starts with no predictors and iteratively adds the most predictive variable.
It stops when the improvement is no longer statistcally significant.

- **Backward** starts with all the predictors and iteratively removes the least predictive variable.
This again stops when there is no longer improvement in the model.

- **Stepwise(both)** this is a combination of forward and backward selections. You start with no variables
like the forward method, but then you also remove any variabels that no longer provide improvement like the backward selection.

### Criticism

Some view this method as mere data dredging, and is a poor substitute for real subject matter expertise. 

### Akaike Information Criterion

Another metric used often is the AIC, it is an estimator of the relative quality of statistical models for a given set of data.
To derive the value the equation is:
$$\text{AIC} = 2k-2ln(\hat{L})$$
where $\hat{L}$ is the maximum value of the likelihood function for the model and $k$ is the number of estimated parameters in the model. The model with the 
lowest AIC value is the preferred model. As you can see AIC favors goodness of fit by the likelihood function and penalizes increasing number of parameters. In a simple linear regression there are three parameters, $\beta_0$ (the intercept), $\beta_1$ (the cofficient) and the variance of the Gaussian distributions. For any least squares model the residuals are expected to be random and normally distributed and counted as one of the parameters.  

```{r}
library(MASS)
stepMod <- stepAIC(condo1, direction = "both")
stepMod$anova
```

```{r}
library(relaimpo)

crlm <- calc.relimp(condo1,
            type = c("lmg", "last", "first"), 
            rela = TRUE )
plot(crlm)
```

### Example with cars data

```{r}
mpg <- mpg %>% mutate_if(is.character, ~as.factor(.x))
carsLm <- lm(hwy~displ+year+cyl+trans+drv+fl+class, mpg)
stepMod <- stepAIC(carsLm, direction = "both")
stepMod$anova
```

```{r}
carsim <- calc.relimp(carsLm,
            type = c("lmg", "last", "first"), 
            rela = TRUE )
plot(carsim)
```


# Generalized Linear Models

+ Logistic Regression can be used when you have a binary regression task.
+ Poisson Regression can be used when count data is being used. 

## Logistic Regression



```{r}
housingBoro <- housing %>% filter(Boro %in% c("Manhattan", "Brooklyn")) %>% 
    mutate(Boro = as.factor(Boro)) %>% 
    dplyr::select(-c("Neighborhood", "Class"))

```

```{r}
parking1 <- glm(Boro ~. , data = housingBoro,
          family = binomial(link="logit"))
summary(parking1)
```

```{r}
coefplot(parking1, sort = 'mag',coefficients = names(parking1$coefficients)[-1])
```


```{r}
parking2 <- glm(Boro ~IncomePerSqFt , data = housingBoro,
          family = binomial)
summary(parking2)

logitPlot <- data.frame(IncomePerSqFt = seq(min(housingBoro$IncomePerSqFt), max(housingBoro$IncomePerSqFt),len=nrow(housingBoro)))
logitPlot$Boro = predict(parking2, newdata=logitPlot, type="response")

housingBoro %>% mutate(Boro = if_else(Boro == "Manhattan", 1, 0)) %>% # have to reclassify manhattan as 1
    ggplot(aes(x=IncomePerSqFt, y=Boro)) + geom_jitter(shape = 1, height = 0.005, alpha = .25) + 
    geom_line(data = logitPlot, aes(x = IncomePerSqFt, y=Boro), colour = "red") +
    theme_minimal()
```

# glmnet

Glmnet is a package that fits a generalized linear model via penalized maximum likelihood. The regularization
path is computed for the lasso or elasticnet penalty at a grid of values for the regularization parameter
lambda.

$$\text{min}_{\beta_0, \beta}\frac{1}{N}\sum ^N _{i=1}w_il(y_i,\beta_0+\beta^Tx_i) +\lambda[(1-\alpha)||\beta||_2^2/2+\alpha||\beta||_1]$$

## Regularization

If you recall the concepts of overfitting and parsimony, simplifying models as much
as possible is beneficial. Regularization is a popular technique for avoiding overfitting
which adds another element to the loss function.


## Ridge Regression

$$L = \Sigma(\hat{Y_i}-Y_i)^2 + \lambda\Sigma\beta^2$$

This loss function has two parts, the first is the squared errors, and the second sums over
the squared $\beta$ and multiples it by a new term $\lambda$. We want to penalize the model
for larger numbers of coeffiencets. 

## Lasso Method

This loss function is slightly different from the previous method

$$L = \Sigma(\hat{Y_i}-Y_i)^2 + \lambda\Sigma|\beta|$$

where you can see the $\beta$ is now a absolute value. Unlike the Ridge method where
coefficients are pushed towards zero, the Lasso will set irelevant coeffiencts to zero.