---
title: "GIS with R"
author: "Kaz Sakamoto"
output: 
    html_document:
        toc: yes
        toc_float:
            collapsed: no
            smooth_scoll: yes
        number_sections: yes
---

<style>
@import url('https://fonts.googleapis.com/css?family=Noto+Sans|VT323');
body{
  font-family: 'Noto Sans', sans-serif;
  font-size: 12px;
  line-height: 24px;
}

h1,h2,h3,h4 {
  font-family: 'VT323', monospace;
}
p.caption {
  font-style: italic;
  font-size: 10px;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, cache = TRUE, message = FALSE)
```

## Spatial Libraries

```{r}
library(dplyr)
library(ggplot2)
library(readr)
library(sf)
library(sp)
library(raster)
library(tmap)
library(spatstat)
library(maptools)
library(tmaptools)
library(spdep)
library(purrr)
##tidyr
library(gstat)
#coefplot
library(spgwr)

## here we are going to find the root_file 
root <- rprojroot::find_rstudio_root_file()
## we are going to set the data file path here
dataDir <- file.path(root, 'Data')
```

# Introduction

## Cartorgaphic History

Geographic knowledge was essential for the survival of early hunter-gathers in
locating their food supply, water, and housing. Humans have been exploring and 
cataloging the world around them, from shared collective memories,
etchings on clay tablets, or storage on modern data bases. We humans may 
no longer need to take inventory of our natural surroundings quite like our 
ancient ancestors, but the need to navigate spatial information has only increased.
A myriad of apps require a geographic location, such as ride sharing apps like Uber
and Lyft, tracking packages from Amazon, and even posting/checking in
on social media like Instagram and Facebook.

```{r clayTablet, echo=FALSE, fig.cap="Imago Mundi: Babylonia 500 BCE"}
knitr::include_graphics('https://www.geolounge.com/wp-content/uploads/2014/11/Baylonianmaps.jpg')
```

```{r googleMaps, echo=FALSE, fig.cap="Google Maps Routes"}
knitr::include_graphics('https://nyc.streetsblog.org/wp-content/uploads/2010/03/08/Picture_2.png')
```

## Data and Maps

Cartography completely changed in 1854 with John Snow's Cholera map. For the first time 
data and summary statistics were combined on to a map. You can see that there is a bar chart
on the streets showing the number of deaths per building. Maps no longer have to solely
function for wayfinding and directions but could be used for analysis.

```{r JohnSnow, echo=FALSE, fig.cap="Broad Street Cholrea Map"}
knitr::include_graphics('https://upload.wikimedia.org/wikipedia/commons/thumb/2/27/Snow-cholera-map-1.jpg/300px-Snow-cholera-map-1.jpg')
```

## GIS

So what is a GIS? 

**A GIS is a computer-based system to aid in the collection, maintenance, storage, analysis, output, and distribution of spatial data and information.**

GIS allows us to define the "where" and "what" types of questions that we have.
Previously GIS was not user-friendly and sequestered in government and academia.
With the advances in data storage, computing power, and graphics cards, GIS
was able to become more widespread heralding an age of increased users. Unlike most tabular 
data, spatial data is still unique which is why GIS is important in unlocking
insights from geography. 

## Spatial Statistics

GIS isn't useful just for visualizing data, and it's end goal is not just making maps.
We can start to answer questions about how features are distributed in space, what kind
of patterns are created by the feature, where clusters might be located, and the 
relationship between a set of features over space. We can also go beyond visual
exploration and inspection but utilize statistics to examine and describe these
spatial phenomena.

```{r spatialStats, echo=FALSE, fig.cap="Spatial Statistics"}
knitr::include_graphics('https://freshbiostats.files.wordpress.com/2013/03/spatial_map2.png')
```


# Working with Geographic Data

## Geographic Data Models

The two most fundamental data models used in geographic data is **vector** and **raster** formats.
If you are familiar with the Adobe Creative Suite programs, **vector** will be analogous
to Illustrator, and **raster** to Photoshop. The vector model uses points, lines, and polygons
to represent spatial features, while the raster model divides the surface into cells or pixels.

You can define spatial data in vector or raster but which to use?
 - Vector data has dominated the social sciences since human settlements tend to have discrete boundaries
 - Raster is utilized most in environmental sciences since remote sensing data comes in as images.

```{r dataModel, echo=FALSE, fig.cap="GIS Data Models"}
knitr::include_graphics(file.path(root, "Images", "gisDataModel.png"))
```

### Vector

The vector model are made up of points, usually lat/long, or x-y pairs. These points
are the fundamentals that create other features such as lines, points connected to 
one another, and polygons where the first point of origin and the end point connect
to enclose the shape. 

```{r representation, echo=FALSE, fig.cap="Phomena vs. Representation"}
knitr::include_graphics(file.path(root, "Images", "SpatialDataRepresentation.png"))
```

### Raster

In spatial analysis, the other type of data model used are rasters. Rasters are
matrices with rows and columns representing equally spaced cells often referred
to as pixels. Each cell has a dimension whether it is 10 ft x 10 ft, 1 m x 1 m which is
known as the cell-size resolution. There is also a header with CRS information,
the extent, and origin (if you recall graphing in high school).

Unlike vector data, cells of a raster can have only one value, and it could represent
continuous or discrete data. 

```{r rasterMap, echo=FALSE, fig.cap="GIS Data Models"}
knitr::include_graphics("https://geocompr.robinlovelace.net/figures/raster-intro-plot2-1.png")
```

### Layers

The power of GIS are all about layers. We are trying to model the real world 
and to do that we create layers of information whether they are modeled as rasters
or vectors to create a simplified but meaningful explanation of the world.

```{r layers, echo=FALSE, fig.cap="Layers in GIS"}
knitr::include_graphics("https://saylordotorg.github.io/text_essentials-of-geographic-information-systems/section_11/ca6ce94cdd2e09a1da8aa6ec22336835.jpg")
```

## Simple Features

There are seventeen geometry types supported by the Open Geospatial Consortium(OGC),
but there are seven that are most commonly used. As you can see these are all vector
formats, as `sf` does not handle raster data. 

```{r simplefeatures, echo=FALSE, fig.cap="Common simple features"}
knitr::include_graphics("https://geocompr.robinlovelace.net/figures/sf-classes.png")
```

The `sf` package relies on the `rgdal` package for reading and writing spatial data and
`rgeos` for spatial operations. 

```{r}
library(spData)

world %>% head()

names(world)
```

```{r}
plot(world)
```

## Coordinate Reference Systems

The coordinate reference system is crucial since it ties the vector and raster data types to a location on the earth (or other bodies). To get this reference system, they rely on geodetic datums which are reference points located all around the earth. The most common datums we use are the WGS 84 and NAD 83. Because the earth is not completely smooth nor spherical, the common datums that we use approximate an ellipsoid, which is due to the fact the earth's rotation and effects of gravity flatten the poles while the equator slightly bulges. Many localities have specific datums or ellipsoid models to use and in the US the NAD 83 is very common. This data will me located in the `ellps` parameter of the `PROJ CRS` library. 

Here are the important features when modeling the earth.
- Actual Earth's surface:  has mountains and oceans.
- Geoid: the shape that the ocean surface would take under the influence of the gravity and rotation of Earth alone, if other influences such as winds and tides were absent.
- ellipsoid: The approximation of the Earth's surface into a more regular shape. 

```{r CRSMods, echo=FALSE, fig.cap=""}
knitr::include_graphics("https://www.aalto.fi/sites/g/files/flghsv161/files/styles/medium/public/2018-06/surfacecomparison.jpg?itok=Zq-ObTTp")
```

Also when you are working with multiple files with different CRSs you will have to transform them to a common CRS or they will not align. You can think of this as trying to do any calculation with data with different units. 

### Geographic Coordinate Systems

Geographic coordinate systems use angular measurements to locate a place on the surface of the earth. These measures are called Latitude and Longitude, they are a measure from the center of earth to the location on the surface. Longitude is the E-W distance from the Prime Meridian plane, while latitude is the N-S distance from the equatorial plane. 

One important aspect to highlight is that these are *angular* measurements. This means that it is not a distance like meters or miles and there fore a distance calculation is not straight forward since we are still talk about a curved surface of the earth and its location.

### Projected Coordinate Systems

Projected coordinate systems are based on Cartesian coordinates if you recall making graphs in math classes with x and y locations. Unlike the angular measurements of the geographic coordinate systems, these are *linear* measurements like meters and feet. Projected coordinate systems are based on geographic coordinate systems but take the 3D object of the earth and project it into 2D. 

This projection will always cause some sort of distortion when you go from a 3D object to a 2D. The classic example is trying to flatten a orange peel, you can't do that with out tearing the skin, flattening and stretching it out. A developable surface is a surface can 
be flattened onto a plane without distortion (i.e. "stretching" or "compressing").

While we many cartographic projections involve projecting the Earth to a developable surface and then "unrolling" the surface into a region on the plane. Since they may be constructed by bending a flat sheet, they are also important in manufacturing objects from sheet metal, cardboard, and plywood. An industry which uses developed surfaces extensively is shipbuilding.^[[Developable Surface](https://en.wikipedia.org/wiki/Developable_surface)]

```{r distortion, echo=FALSE, fig.cap="An orange is not considered a developable surface"}
knitr::include_graphics("https://learngis.org/Images/smashed_orange01.jpg")
```

There are certain properties that we want to accurately capture when we are working with mapping, these are area, distance, direction, and shape. A projected coordinate can only preserve one or two properties, because of this the map maker must think about the trade-offs being made when projecting the earth on to a 2D map. This is apparent when you think about the debate between the "Euro-centric" Mercator projection that we have all become accustomed to compared with the Peters Projection that accurately displays areas. 

```{r Projections2, echo=FALSE, fig.cap="Maps can be Political"}
knitr::include_graphics("https://external-preview.redd.it/6Vagn3BQ1Mwh4kN2-uldK7X8UM9_QAvYJCN2CCvoLWs.png?auto=webp&s=ca818d0e1049c02e6c0e82bf9223250822f30565")
```

Tissot's Indicatrix is a way of visualizing some of these distortions. It can
illustrate linear, angular, and areal distortions. 

```{r Tissot1, echo=FALSE, fig.cap="Mercator Projection"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/8/87/Tissot_mercator.png")
```

```{r Tissot2, echo=FALSE, fig.cap="Equidistant Projection"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/8/87/Azimuthal_equidistant_projection_with_Tissot%27s_indicatrix.png")
```

```{r Tissot3, echo=FALSE, fig.cap="Equal Area Projection"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/e/e7/Tissot_indicatrix_world_map_Gall-Peters_equal-area_proj.svg")
```

The last thing to cover is the three main groups of projections types which are conic, cylindrical and planar or azimuths. As you can probably infer by their names these are the shapes that are being used to capture the projection, if you think of earth with a light bulb in the middle and place these objects on the surface of the earth, you can see that the earths surface would be projected on the shape. Distortions are minimized where the projection shape is tangent with the earths surface and greater the further you get from it. These shapes can either create one line of tangency or secant along two lines.

```{r lightsource, echo=FALSE, fig.cap="Light bulb projection"}
knitr::include_graphics("https://www.acsu.buffalo.edu/~dbertuca/maps/cat/cylindrical_final.gif")
```

```{r projectionSurface, echo=FALSE, fig.cap=""}
knitr::include_graphics("http://gisedu.colostate.edu/webcontent/nr505/2012_Projects/Team6/images/GIS_concepts/Figure2_proj.PNG")
```

Intersections of the projection surface usually only happen once or twice, these terms are tangents and secants. In geometry, a secant of a curve is a line that intersects the curve in at least two (distinct) points.

```{r secantLine, echo=FALSE, fig.cap="Common lines and line segments on a circle"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/CIRCLE_LINES.svg/1280px-CIRCLE_LINES.svg.png")
```

Here's another graphic showing that if the cylinder fits just around the equator, there is a point of tangency. On the other hand,
if the developable surface is smaller, some of it will be inside of the object we are trying to project which makes it secant where it
intersects the object twice.
```{r projectionObjects, echo=FALSE, fig.cap=""}
knitr::include_graphics("http://geokov.com/Images/coordref/cylindrical-projection.png")
```

Here are more diagrams showing secant projections. You'll see the points where the projection
surface(in this case a cone or cylinder) create these bands called Standard Parallels. These lines
are where there are no distortions along the projection. Between the parallels, these is a negative
distortion, meaning that there is more object than space on the developable surface, while outside of the
parallels there is positive distortion because there is less object for the surface and gets blown up. 

```{r secants, echo=FALSE, fig.cap=""}
knitr::include_graphics("https://www.e-education.psu.edu/geog862/sites/www.e-education.psu.edu.geog862/files/images/Lesson06/Decreasing%20Distortion.png")
```

and just for fun, you can see people really get into projections!

```{r xkcdProjections, echo=FALSE, fig.cap=""}
knitr::include_graphics("https://imgs.xkcd.com/comics/map_projections.png")
```

## Working with CRSs

You can work with the CRS of geographic data with `epsg` codes or `proj4string` definitions. `epsg` codes are shorter and might be easier to remember while the `proj4string` will allow you more flexibility when dictating projections type, datum, and ellipsoid parameters. `epsg` only refers to one specific CRS which does not allow you to change different parameters. 


```{r}
nybb <- read_sf(file.path(dataDir, "gis/nybb"))
st_crs(nybb)
st_crs(nybb) <- 2263
st_crs(nybb)
```

```{r}
centralPark <- data.frame(lon = -73.9691305, lat = 40.7764627) %>% 
  st_as_sf(coords = c("lon", "lat"))
st_is_longlat(centralPark)
st_crs(centralPark)
```

```{r}
centralPark_geo <- st_set_crs(centralPark, 4326)
st_is_longlat(centralPark_geo)
```

Here you can see that only the second data set created a warning. We are trying to run a distance based
function, creating a buffer around a data with latitude and longitude
. 
```{r}
centralPark_buff_no_crs <- st_buffer(centralPark, dist = 1)
centralPark_buff <- st_buffer(centralPark_geo, dist = 1)
```

If you try to map our borough boundary and central park, it will luckily 
assume that the CRS is for WGS84, but you cannot rely on this!

```{r, warning=TRUE}
tm_shape(nybb) + tm_polygons() + 
tm_shape(centralPark) + tm_symbols() 
```

Here with the properly encoded geometry it won't give us any warnings.
They are still different CRSs one projected and one geographic so you won't be able
to make any linear calculations across them.

```{r}
tm_shape(nybb) + tm_polygons() + 
tm_shape(centralPark_geo) + tm_symbols() 
```

# Mapping Basics

Making charts with your regular data, so naturally making maps with your
spatial data is just as crucial. Here we will dive into two methods or mapping,
static maps and interactive maps. While interactive maps are great, static maps
are still a powerful and should be thought of as curated experiences for your audience.

Outside of the static vs. interactive paradigms there are many types of maps that are created.

Here are a sample:

- Physical maps show landscape features like rivers and land masses
- Topographic maps show the contours of elevation lines
- Climate maps can show atmospheric conditions
- Thematic maps show special topics like election results or even energy usage in areas.

```{r barometricPressure, echo=FALSE, fig.cap="Barometric Pressure"}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/January_17_1982_500-Millibar_Height_Contours.png/1280px-January_17_1982_500-Millibar_Height_Contours.png")
```

## Mapping Elements

Here is a list of items that should be added to each map. How we will be using
maps are for thematic mapping and should be thought of more as data visualization.
This doesn't mean graphic design principles should be thrown out the door, but
as much ambiguity should be removed from the product. 

- Title : Should clearly introduce the topic of the map
- Legend : Gives an understanding of the range, and variation in data (make the units easy to read)
- North Arrow : Offers orientation for cardinal directions
- Scale Bar : Shows the scale whether it is large or small scale map and amount of detail to be excepted.^[[Scale](https://en.wikipedia.org/wiki/Scale_(map))]
- Sources : Give credit where credit is deserved
- Date : It's a good practice to date your maps so you remember when it was made
- Name : Also don't forget to take credit for your work

```{r MapElements, echo=FALSE, fig.cap="Elements of a basic map"}
knitr::include_graphics("https://saylordotorg.github.io/text_essentials-of-geographic-information-systems/section_13/0c71cd48743695df3cb5ca49fd858393.jpg")
```

## Static Maps

Static maps are your traditional maps that you may find in an atlas, a poster, or
in a report. As they say if "A picture is worth a thousand words", I would go further
and say that "A map is worth ten thousand words". It's an information rich document
and it's the deft hand of the creator to dictate how much detail and data needs to 
be added to get your point across. 

As I say in my classes, you have to be a good map consumer to be a good map producer.
Not all maps are created equal and honing a critical eye towards good data visualization
and graphic design will take you far. Also once you create your aesthetic whether it's 
more modern, or traditional looking, it'll be easier to stick with it. 

```{r mapDesign, echo=FALSE, fig.cap="Modern map making example | Center for Spatial Research"}
knitr::include_graphics("http://c4sr.columbia.edu/sites/default/files/styles/project-page-image/public/Green_Origins-01.png?itok=X1-dgsUv")
```

### tmap

tmap is based on the grammar of graphics, just like ggplot2. We can think of mapping
our geometries to the input data and string them together to make sentences with `+`.

As a caveat tmap can also be used for interactive maps with `tmap_mode("view")` but I will be showing you
how to use it for static mapping which is `tmap_mode("plot")`. 

Like the `ggplot` object, we first start with the shape,
then you specify how you want to represent the geometry.

```{r, eval = FALSE}
# create borders
tm_shape(nybb) +
  tm_borders() 

# create the fill
tm_shape(nybb) +
  tm_fill() 

# combine them
tm_shape(nybb) +
  tm_fill() +
  tm_borders() 

# or polygons
tm_shape(nybb) + 
    tm_polygons() 
```

```{r, echo=FALSE}
# create borders
tm_shape(nybb) +
  tm_borders() +
    tm_layout(title = "tm_shape(nybb) + 
              tm_borders()")

# create the fill
tm_shape(nybb) +
  tm_fill() +
    tm_layout(title = "tm_shape(nybb) + 
              tm_fill()")

# combine them
tm_shape(nybb) +
  tm_fill() +
  tm_borders() +
    tm_layout(title = "tm_shape(nybb) +
              tm_fill() + 
              tm_borders()")

# or polygons
tm_shape(nybb) + 
    tm_polygons() +
    tm_layout(title = "tm_shape(nybb) + 
              tm_polygons()")
```

### Aesthetic Mappings

Here is how `tmap` has organized their Aesthetic mappings

Aesthetics base layers:

- `tm_polygons` Create a polygon layer (with borders)
- `tm_symbols` Create a layer of symbols
- `tm_lines` Create a layer of lines
- `tm_raster` Create a raster layer
- `tm_text` Create a layer of text labels
- `tm_basemap` Create a layer of basemap tiles
- `tm_tiles` Create a layer of overlay tiles

Aesthetics derived layers:

- `tm_fill` Create a polygon layer (without borders)
- `tm_borders` Create polygon borders
- `tm_bubbles` Create a layer of bubbles
- `tm_squares` Create a layer of squares
- `tm_dots` Create a layer of dots
- `tm_markers` Create a layer of markers
- `tm_iso` Create a layer of iso/contour lines
- `tm_rgb` Create a raster layer of an image

Working with colors requires specifying the `col` argument, here we specify
a color by name. We can also change the line width, or line type. 

```{r}
map1 <- tm_shape(nybb) + tm_fill(col = "red")
map2 <- tm_shape(nybb) + tm_fill(col = "red", alpha  = .2)
map3 <- tm_shape(nybb) + tm_borders(col = "grey")
map4 <- tm_shape(nybb) + tm_borders(col = "blue", lwd = 0.5, lty = 2)
tmap_arrange(map1, map2, map3, map4)
```

We can also use a variable(either continuous or categorical) to change the color too.

```{r}
tm_shape(nybb) + tm_fill(col = "BoroName") + 
    tm_borders(col = "blue", lwd = 0.5, lty = 2)
```

Layout elements:

- `tm_layout` Adjust the layout (main function)
- `tm_legend` Adjust the legend
- `tm_view` Configure the interactive view mode
- `tm_style` Apply a predefined style
- `tm_format` Apply a predefined format

```{r}
tm_shape(nybb) + tm_fill(col = "BoroName", title = "Boroughs") + 
    tm_borders(col = "white", lwd = 0.5) + 
    tm_text(text = "BoroName", size = 0.7, col = "black") + 
    tm_layout(title = "NYC", bg.color = "lightblue")
```

Attributes:

- `tm_grid` Create grid lines
- `tm_scale_bar` Create a scale bar
- `tm_compass` Create a map compass
- `tm_credits` Create a text for credits
- `tm_logo` Create a logo
- `tm_xlab` and tm_ylab Create axis labels
- `tm_minimap` Create a minimap (view mode only)

```{r}
tm_shape(nybb) + tm_fill(col = "BoroName", title = "Boroughs") + 
    tm_borders(col = "white", lwd = 0.5) + 
    tm_text(text = "BoroName", size = 0.7, col = "black") + 
    tm_layout(title = "NYC", bg.color = "lightblue") +
    tm_compass() + 
    tm_scale_bar() + 
    tm_credits("New York City Department of City Planning 5/20/2019",
               size = 0.5,align = "right")
    
```

### Affine Transformation

What happens when you need to rotate your map? Affine transformation to the rescue. Here
we will use a matrix to help us rotate the map by radians(converted from degrees).

This might be important if you are used to seeing geographies in a particular orientation
such as Manhattan. So even though Manhattan is not perfectly vertically aligned with North
a lot of depictions show it as such.
^[[Affube Tranformations](https://www.mathworks.com/discovery/affine-transformation.html)]

```{r}
rot <- function(radians){
    matrix(c(cos(radians), sin(radians), 
             -sin(radians), cos(radians)), 2, 2)  
} 

radianConv <- function(degrees){
    (degrees /180) * pi
}                                         
nybbg <- st_geometry(nybb)
nybbd <- st_drop_geometry(nybb)
nybbg2 <- (nybbg) * rot(radianConv(331))
nybbg2 <- st_set_crs(nybbg2, 2263)
nybbRot <- st_sf(nybbg2, nybbd)
tm_shape(nybbRot) +  tm_fill(col = "BoroName", title = "Boroughs") + 
    tm_borders(col = "white", lwd = 0.5) + 
    tm_text(text = "BoroName", size = 0.7, col = "black") + 
    tm_layout(title = "NYC", bg.color = "lightblue") +
    tm_compass(north = 331) + 
    tm_scale_bar() + 
    tm_credits("New York City Department of City Planning 5/20/2019",
               size = 0.5,align = "right")
```


## Interactive Maps

There are times when interactive maps are more beneficial than having a static map.
In my experience, interactive mapping is best used when exploration might be needed
by the user. Unlike a static map experience, zooming, panning, clicking can be utilized
to explore your output.

### leaflet

Leaflet is a great interactive mapping platform and couldn't be simpler to add to your
rmarkdown document/slides or shiny dashboards. It can deal with `sf` and `sp` objects
so you don't have to make any conversions with your spatial data!

Here's a basic example using leaflet, first load the library. Than

```{r}
library(leaflet)
m <- leaflet() %>% 
    setView(lng = -73.9691305, lat = 40.7764627, zoom = 11) %>% 
    addProviderTiles(providers$CartoDB.Positron)
m

```

# Exercise 1 | Areal Mapping 

Now we will see what the consequences are of using different projections when we
are calculating area. We will use the Equal Area projection as our standard since
we know that we are preserving area with this type of projection. We will compare this
against the Equal Distance, Conformal, and State Plane projections. 

- [Equal Area](https://epsg.io/102008)
- [Equal Distance](http://epsg.io/102005)
- [Small Shapes Maintained](http://epsg.io/102004)
- [State Plane](https://epsg.io/2263)

```{r}
Area_Albers <- 102008
Area_Equidistant <- 102005
Area_Lambert <- 102004
Area_NYStatePlane <- 2263
projections <- setNames(c(Area_Albers, Area_Lambert,Area_Equidistant, Area_NYStatePlane),
         c("Area_Albers", "Area_Lambert","Area_Equidistant", "Area_NYStatePlane")) %>% 
    as.list()
```

We will now read in our usCounties shapefile using the `st_read` function.

```{r}
usCounties <- st_read(file.path(dataDir, "gis/us_county"),stringsAsFactors = FALSE)
```

We will now create a function to calculate our areas and map through them. You have to
be careful of what units the projections are and some of them are in meters, while
some of them are in feet. Here we will convert the square meters areas to square feet,
and the conversion is below:

- 1 m^2  = 10.7639 ft^2

```{r}
massReproject <- function(shape, projection){
    # we have to reporject and recalulate the area
    st_transform(shape, projection) %>% st_area()
}

areaCalc <- purrr::map_df(projections, ~massReproject(usCounties, .x)) %>% 
    mutate_at(vars(Area_Albers, Area_Equidistant, Area_Lambert), function(x) x*10.7639) %>% 
    mutate(GEOID = usCounties$GEOID) %>% 
    mutate(Albers_Diff = Area_Albers-Area_Albers,
           Lambert_Diff = Area_Albers- Area_Lambert,
           Equidistant_Diff = Area_Albers-Area_Equidistant,
           NYStatePlane_Diff = Area_Albers-Area_NYStatePlane)
```

We'll join our new calculation to our sf object.

```{r}
usCounties <- usCounties %>% left_join(areaCalc %>% dplyr::select(5:9))
```


You can add a projection on the fly. by specifying the `projection` attribute
in tm_shape too. 

Lambert Conformal Conic:

```{r}
tm_shape(usCounties, projection = 2163, unit = "mi") + 
    tm_polygons(col = "Lambert_Diff", lwd = 0.1, palette = "RdBu", n = 7, midpoint = 0, style = "fisher") +
        # tm_compass(position = c("right","bottom"),
    #            text.color = "grey", 
    #            color.dark = "grey",
    #            size = 1) +
    tm_scale_bar(position = c("center", "bottom"),
                 text.color = "grey",
                 color.dark = "grey") +
    tm_legend(position=c("left", "bottom"),
              just = "right") + 
    tm_layout(inner.margins = c(0.1,0.08,0.01,0.08))

```

Equidistant:

```{r}
tm_shape(usCounties, projection = 2163, unit = "mi") + 
    tm_polygons(col = "Equidistant_Diff", lwd = 0.1, palette = "RdBu", n = 7, midpoint = 0, style = "fisher") + 
    # tm_compass(position = c("right","bottom"),
    #            text.color = "grey", 
    #            color.dark = "grey",
    #            size = 1) +
    tm_scale_bar(position = c("center", "bottom"),
                 text.color = "grey",
                 color.dark = "grey") +
    tm_legend(position=c("left", "bottom"),
              just = "right") + 
    tm_layout(inner.margins = c(0.1,0.08,0.01,0.08))
```

NY State Plane:

```{r}
tm_shape(usCounties, projection = 2163, unit = "mi") + 
    tm_polygons(col = "NYStatePlane_Diff", lwd = 0.1,palette = "RdBu", n = 4, midpoint = 0, style = "fisher") + 
    # tm_compass(position = c("right","bottom"),
    #            text.color = "grey", 
    #            color.dark = "grey",
    #            size = 1) +
    tm_scale_bar(position = c("center", "bottom"),
                 text.color = "grey",
                 color.dark = "grey") +
    tm_legend(position=c("left", "bottom"),
              just = "right") + 
    tm_layout(inner.margins = c(0.1,0.08,0.01,0.08))
```


# Attribute data operations

the `sf` library is great because it extends the data.frame and adds geographic features to it. Much like the tidy data
paradigm, each row is still an observations and each column is a feature. The main difference between a normal data.frame 
and an sf object is that there is another column `geometry` baked in which can contain a multitude of geographic types
like points, lines and polygons. 

You will see that a lot of methods that you are used to working with in data.frames will apply to sf objects. Like `rbind`,
`$<-`, and `cbind`.
```{r}
methods(class = "sf")
```

If we ever wanted to go back to a normal data.frame object it's very simple with `st_drop_geometry`

```{r}
st_drop_geometry(nybb) %>% class()
```

## Subsetting

Subsetting attribute data will be done in the same ways you have been dealing
with `data.frame` object types. The first way of doing it is through the `[` 
notation. The left-hand side represents the rows and the right-hand side columns.
```{r}
nybb[1,]
nybb[,c(2,4)]
```

If you like the dplyr verbs, `slice` and `select` will also work. This is the
convenience of storing data in an `sf` object.

```{r}
nybb %>% slice(1:2)
nybb %>% dplyr::select(2)
```

As with filtering data, logical operations can be used to subset data as well. 
Below we can filter on areas greater than 1,937,566,944 sqft. 

```{r}
nybb %>% filter(Shape_Area >= 1937566944)
```


## Joining data

For storage purposes, most geographic data does not come with much extra attribute data.
Therefore joining in data with unique key is going to be important. You'll have to make sure
that there is a geographic ID or key that you can use in your data. For instance
if you are working with Census information there are the [FIPS code](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standards), or 
zipcodes, or unique state names all work.


We're going to scrape some information from Wikipedia on the boroughs of NYC.

```{r}
library(rvest)
demographics <- read_html("https://en.wikipedia.org/wiki/Demographics_of_New_York_City") %>% 
    html_nodes(xpath = '//*[@id="mw-content-text"]/div/table[1]') %>% 
    html_table(header = FALSE) %>% "[["(1) %>% slice(4:8) %>% 
    mutate(X1 = c("Bronx", "Brooklyn", "Manhattan", "Queens", "Staten Island")) %>% 
    dplyr::select(c(1,3:5))

names(demographics) <- c("BoroName", "population",
                         "GDP", "GDPperCapita")
```

We'll create a new variable called `nybb_demo` and join in our table we just scraped.
There is a little bit of cleaning to be done too.

```{r}
nybb_demo <- nybb %>% left_join(demographics) %>% 
    mutate_at(vars(population, GDP, GDPperCapita), function(x) stringr::str_remove_all(x, ",") %>% as.numeric())
```

Now we can make a [choropleth map](https://en.wikipedia.org/wiki/Choropleth_map)
with our population column. 

```{r}
tm_shape(nybb_demo) + tm_polygons(col = "population")
```


# Spatial Operations

The main reason we want to use GIS is for spatial operations or geoprocessing tools.
Before we working with attribute data, but we will be selecting and filtering
based on their location and shape. Common tools are intersects, clip, spatial joins,
and buffers. 

## Intersects

Now we will load in Capital Improvement Projects data from the NYC Department of Education.
You'll notice that we are reading in normal csv first and then casting it as an sf
object with the `Longitude` and `Latitude` columns in the data.frame.

```{r}
cip <- read_csv(file.path(dataDir, "CIP2019.csv"))

cip %>% head()

cip <- cip %>% arrange(`Building ID`,Latitude) %>% tidyr::fill(Latitude, Longitude, .direction = "down") %>% 
    group_by(`Building ID`, Latitude, Longitude) %>% summarise(Award = sum(`Construction Award`, na.rm = TRUE)) %>% 
    ungroup()

cipSF <- st_as_sf(cip, coords = c("Longitude", "Latitude"),crs = 4326)

cipSF <- cipSF %>% filter(Award > 0)

cipSF <- st_transform(cipSF, 2263) 
```

let's subset just Brooklyn so we can use it as our selection criteria.

```{r}
BK <- nybb %>% filter(BoroName == "Brooklyn")
```

and we can see what it looks like. 

```{r}
tm_shape(nybb) + tm_polygons(col = "white") +
    tm_shape(BK) + tm_fill(col = "grey")+
    tm_shape(cipSF) + tm_symbols(size = .2, col = "purple", shape = 1) 
```

Let's see which cipSF intersects our BK geometry. You'll see that there are either
1 for every intersection or empty values from our sparse matrix, this means if that if any spatial relationship
between these two inputs are expressed it'll be indicated once.

```{r}
cipBKsparse <- st_intersects(cipSF, nybb)
cipBKsparse
```

A dense matrix can be created by adding the `sparse = FALSE` argument. 

```{r}
cipNY <- st_intersects(cipSF, nybb, sparse = FALSE)
cipNY %>% head()
```


## Disjoint

The opposite of `st_intersects()` is disjoints.  Here we can the dense matrix
where the geometries do not spatially relate. 

```{r}
st_disjoint(cipSF, nybb, sparse = FALSE) %>% head()
```

## Within

While intersect is kind of the catch-all of spatial relationships,
you can be more specific. within checks if x is within y.  
```{r}
st_within(cipSF, nybb, sparse = FALSE) %>% head()
```

## Within Distance

if you want to add a search distance with your spatial query, `st_is_within_distance`
is your tool. You can add a dist argument(remember what your units are) and 
go and expand your search.

```{r}
sel <- st_is_within_distance(cipSF, nybb, dist = 100, sparse = FALSE)
sel %>% head()
```

## Spatial Joining

sometimes, you want to add attribute data from one geometry to another. In a normal
join we did it with a unique key, but a spatial join will happen with intersect. 
It will be a left or inner join type. 

```{r}
cipJoin <- st_join(cipSF, nybb_demo)
cipJoin
```

```{r}
tm_shape(nybb) + tm_polygons(col = "white")+
    tm_shape(cipJoin) + tm_symbols(size = .2, col = "BoroName", 
                                    alpha = 0.4) 
```

## Spatial Data Aggregation

```{r}
cipJoin %>% aggregate(by = nybb, FUN = mean)
```

## Distances

When we were looking at topological relationships, the results were only binary, either they
intersect or does not, etc...

```{r}
nybb_centroid <- st_centroid(nybb)

st_distance(cipSF[1,], nybb_centroid)
```

```{r}
st_distance(cipSF %>% slice(1:10), nybb_centroid)
```

```{r}
tm_shape(nybb) +tm_polygons(col = "white") + 
tm_shape(nybb_centroid) + tm_symbols(col = "blue", size = .2) + 
tm_shape(cipSF %>% slice(1:10)) + tm_symbols(col = "red", size = .2, shape = 1)
```

# Geometric Operations

So far you have worked with attribute data, and spatial operations. One we thing 
we haven't really done is work with the geometric operations or the shapes that
make up the shapefile. These operations are also unique because their outputs are
new geoemtires!

## Simplification

Remember that all of this data is generated by hand, where someone painstakingly
hand-coded these vectors from often satellite imagery or historic maps. The level
of detail might be greater than you would like sometimes and generalizing these
geometries might be needed. This helps to reduce the memory it takes to store
these complex objects too. 

The `st_simplify` will help you remove some of the detail from a shape. One argument
you can play around with is the `dTolerance` which will be the distance in the 
projection's units that will be tolerated. Here is the behind the scenes explanation:

*This is done by selecting the endpoints of the line and the point farthest from the arc joining the end points, known as the “worst point”. If the worst point is farther from the joining arc than the tolerance value, it is kept and the substrings between the start and worst points, and worst and end points are considered recursively. In this way, a linestring comprised of enough of the original points can be produced that is “similar” to the original line. The units of tolerance are the same as the projection of the input geometry.*

```{r}
nyc_simp <- st_simplify(nybb, dTolerance = 500)
plot(nyc_simp["BoroName"])
plot(nybb["BoroName"])
```

Now we can compare the space allocation

```{r}
object.size(nybb)
object.size(nyc_simp)
```

The issue with our previous function is that topology between geometries
are not preserved. The mapshaper version will actually preserve the
relationship between geometries with the `keep_shapes` argument set to TRUE.

```{r}
nyc_ms <- rmapshaper::ms_simplify(nybb, keep = 0.01,
                                          keep_shapes = TRUE)
```

```{r}
tm_shape(nyc_ms) + tm_polygons()
```

the sf implementation uses the [Ramer-Douglas-Peucker Algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm),
while the rmapshaper uses the [Visvalingam-Whyatt Line Simplification Algorithm](https://hull-repository.worktribe.com/output/459275)

Here is a [great explanation](https://bost.ocks.org/mike/simplify/) with example.
This is a [cool comparisson](http://bl.ocks.org/msbarry/9152218)

## Centroids

Centroids are also known as the geometric center which is the arithmetic mean position of all the points that consist of a geometry. 
You can think of it as the place where if you were to put your finger on the spot it 
would balance perfectly.

To create the centroid use the `st_centroid` function.

```{r}
nybb_centroid <- st_centroid(nybb)

tm_shape(nybb) + tm_polygons()+
    tm_shape(nybb_centroid) + tm_symbols() 
```

There are times when the centroid falls outside of the shape. For polyline features
this is often the case. 

Let's load in the subway lines data

```{r}
subway <- read_sf(file.path(dataDir, "gis", "SubwayLines")) %>% 
    st_transform(2263)
tm_shape(nybb) + tm_polygons(col ="white")+
tm_shape(subway) + tm_lines()
```

We will do some cleaning first. Then the `st_point_on_surface` will create
the centroid along the original shape.

```{r}

subwayRT <- subway %>% pull(rt_symbol) %>% unique()

splitSubway <- subway %>% tidyr::nest(-rt_symbol) %>% 
    dplyr::mutate(combine = purrr::map(data, st_combine))

subwayLines <- splitSubway$combine %>% purrr::reduce(c) %>%
    st_sf() %>% mutate(lines = subwayRT)

subwayCentroids <- st_centroid(subwayLines)
subwaysurface <- st_point_on_surface(subwayLines)
```

We can compare the differences in this new map.

```{r}
tm_shape(nybb %>% filter(BoroName != "Staten Island")) + tm_polygons(col = "white")+
tm_shape(subwayLines) + tm_lines(col = "lines") +
    tm_shape(subwayCentroids) + tm_symbols(col = "lines", size =0.5, shape = 3)+
    tm_shape(subwaysurface) + tm_symbols(col = "lines", size = 0.5, shape = 5)
```

## Buffers

Sometimes you need to create a radius around your original geometry. Maybe you need a
1 mile buffer around a nuclear power plant to see what is in the vicinity of the plant?
Or maybe you need to make a catchment area for how many school age children live in 
the intersection of a half-mile from the school and a half-mile from a public library?

To use the function call `st_buffer` with the specified distance.

```{r}
subwayQuarterBuffer <- st_buffer(subwayLines, dist = 1320)
subwayQuarterBuffer
```

```{r}
tm_shape(nybb %>% filter(BoroName != "Staten Island")) + tm_polygons(col = "white") +
tm_shape(subwayQuarterBuffer) + tm_polygons(col = "lines", alpha = 0.2) 
```

## Clipping

clipping is a form of spatial subsetting except kind of like a cookie cutter you will be 
extracting the portion you would like to keep. Here are some common uses for
clipping.

```{r clipping, echo=FALSE, fig.cap="Geometric Verbs"}
knitr::include_graphics("https://geocompr.robinlovelace.net/figures/venn-clip-1.png")
```


## Selecting with Intersect

If you want to create a new geometry from the selection `st_intersection` will
create a new object for you.

```{r}
bkSelect <- st_intersection(x = cipSF, y = BK)
```

```{r}
tm_shape(nybb) + tm_polygons() + 
    tm_shape(bkSelect) + tm_symbols(size = 0.2, shape = 1, col = "purple")
```


## Selecting with Difference

If you want to create a new geometry from the selection `st_intersection` will
create a new object for you.

```{r}
bkDiff <- st_difference(x = cipSF, y = BK)
```

```{r}
tm_shape(nybb) + tm_polygons() + 
    tm_shape(bkDiff) + tm_symbols(size = 0.2, shape = 1, col = "purple")
```

# Raster Analysis

## Creating Rasters

A RasterLayer can easily be created from scratch using the function `raster`.
The default settings will create a global raster data structure with a 
longitude/latitude coordinate reference system and 1 by 1 degree cells. You can
change these settings by providing additional arguments such as xmin, nrow,
ncol, and/or crs, to the function. You can also change these parameters after
creating the object. If you set the projection, this is only to properly define it,
not to change it. To transform a RasterLayer to another coordinate reference
system (projection) you can use the function projectRaster

let's rasterize the CIP data
First we must convert our sf object into an sp object. 
```{r}
cipSP <- as(cipSF, "Spatial")
cipSP <- spTransform(cipSP, CRS("+init=epsg:2263"))
```

We will no create an empty raster object named `rast`,
we'll also add the CRS, extent, and dimensions.
```{r}
rast <- raster(crs =  CRS("+init=epsg:2263"))

extent(rast) <- extent(nybb)
ncol(rast) <- 100
nrow(rast) <- 100
```

we will use `rasterize` to add a cipSP data into the empty raster

```{r}
rast2 <- rasterize(cipSP[,"Award"], rast, field =cipSP$Award, fun=sum)

tm_shape(rast2) + tm_raster()
```


## Extract

We can extract data from our raster back to our points with the `extract` function.

```{r}
cipSF$raster <- raster::extract(rast2, cipSF)

tm_shape(nybb) + tm_polygons(col = "white")+
    tm_shape(cipSF) + tm_symbols(size = .2, col = "raster") 
```

# Point Pattern Analysis

It's one thing to map our points of interest and eyeball some patterns. The real
interesting part is the statistics behind the data. 

## Centrography

The foundations of point pattern analysis is the summary statistics that we
can collect from our points. Much like normal descriptive stats we can get the
mean center, standard distance and standard deviational ellipse if your points may 
have a non uniform trend. 

```{r centrography, echo=FALSE, fig.cap="Central Measures"}
knitr::include_graphics("https://mgimond.github.io/Spatial/img/centrography.svg")
```

First we will convert our sp object to a ppp object.
```{r}
cipPPP <- as(cipSP["Award"], "ppp") 
Window(cipPPP) <- as(nybb,"Spatial") %>% as("owin")
```

```{r}
aspace::calc_sdd(id = 1, filename = file.path(dataDir, "sdd.txt"), points = data.frame(cipPPP$x, cipPPP$y))
```

You can see that our standard distance is 41,324 feet where roughly 68% of our data will be covered in from
the center which is (1006143,201747.6).

Other uses:

- compare two or more distributions
- compare the same feature but over time

## Global Density

if we look at the whole study area we can calculate the point density as $\frac{n}{a}$
where $n$ is the observed number of points and $a$ is the study region's area. For
instance if we had 10 schools in 100 acres, our point density is $\frac{10}{100} = 0.1$
schools per acre. 

## Local Density

We looked global density, but what about point patterns that cane be measured across different location within the study region?
We can then contrast if density is equal across the region, or if there maybe an underlying 
local process at play. 

## Quadrat Density

The first method uses quadrats or sub-divisions, these are often squares, but many other
shapes such as hexagons and triangles can be implored. 

We can first create a 3x3 quadrat using the `quadratcount` function which will divide
up our schools and count up the number of schools. Also quadrat areas don't have to be uniform, it could
be created on a covariate like temperature, landuses, etc by creating a [tessellated surface](http://wiki.gis.com/wiki/index.php/Tessellation).

```{r}
quad <- quadratcount(cipPPP, nx = 3, ny = 3)
```

Now we can explore the schools with our quadrat.
```{r}
plot(cipPPP, cols = "light blue")
plot(quad, add = TRUE)
```

or you could also do it by borough by spatially joining the boro info,
doing group_by on boro, and summarizing the data.

```{r}
cipBoro <- st_join(nybb, cipSF) %>% group_by(BoroName) %>% 
    summarise(schools = n(),
              area = first(Shape_Area),
              awards = sum(Award),
              density = schools/(area/43560), # convert to acres
              awardDens = awards/(area/43560)) %>% 
    arrange(desc(density))

cipBoro

tm_shape(cipBoro) + tm_polygons(col = "density", title = "School Density per Acre")
tm_shape(cipBoro) + tm_polygons(col = "awardDens", title = "Award Density \n(Dollars per Acre)")
```

## Kerenel Density

The kernel density is a method for exploring density from sampled features. It isn't 
too different from the quadrat method since we are still calculating a local density,
but in the kernel method there is a moving window that defines what local is.
A circular search area is applied that determines the distance to 
search for sample locations to spread values around the
locations. If you want to learn more about kernel's here is the [wikipedia link](https://en.wikipedia.org/wiki/Kernel_(statistics)#Nonparametric_statistics)

If you remember Minesweeper a kernel density map will make a lot of sense.

```{r kernel, echo=FALSE, fig.cap="Kernel Density"}
knitr::include_graphics('https://mgimond.github.io/Spatial/11-Point-Patterns_files/figure-html/f11-kernel01-1.png')
```

We will now calculate the density and plot it out with the default values. we are
using the `density` function from `spatstat`. The default kernel is the "Gaussian smoothing kernel"

$$(u) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}u^2}$$

and the default sigma is the standard deviation of the distribution. For more info about
Gaussian smoothing check out this [link](https://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm).

```{r}
cipKerneldefault <- density(cipPPP)
plot(cipKerneldefault, main=NULL, las=1)
raster::contour(cipKerneldefault, add = TRUE)
```


We can specify the `sigma` value, here we will use 5000 ft, larger sigmas will produce
smoother and more generalized density rasters. 

```{r}
par(mfrow=c(1,2))
cipKernel5000 <- density(cipPPP,sigma = 5000)
plot(cipKernel5000, main="Non-Weighted", las=1)
raster::contour(cipKernel5000, add = TRUE)
```

We could also add a weight with the amount of money each school
was awarded with the `weights` field.

```{r}
cipKernelWeighted <- density(cipPPP,sigma = 5000, weights = cipSF$Award)
plot(cipKernelWeighted, main="Weighted", las=1)
raster::contour(cipKernelWeighted, add = TRUE)
```

```{r echo=FALSE}
par(mfrow=c(1,1))
```

tmap also has it's own function to make a kernel density map using `smooth_map`

```{r}
cip_smooth <- tmaptools::smooth_map(cipSF, smooth.raster = TRUE, nlevels = 10, cover = nybb)

tm_shape(cip_smooth$polygons) +
    	tm_fill("level", palette=cm.colors(11), title="CIP Density") +
    	tm_shape(cip_smooth$iso) +
    	tm_iso(col = "grey", size = 0.1) +
    	tm_legend( position=c("left", "top"), bg.color="white", frame = FALSE)
```

if you already have a raster, we can input that is with the `smooth.raster` as
`FALSE`. 

```{r}
kernelRast <- raster::raster(cipKernelWeighted)
crs(kernelRast) <- "+init=epsg:2263"

cip_smoothRast <- tmaptools::smooth_map(kernelRast, smooth.raster = FALSE, nlevels = 10,
                                        cover = nybb)

tm_shape(cip_smoothRast$raster$layer) +
    	tm_raster( palette=topo.colors(5), title="CIP Density",
    	           alpha = 0.5) +
    	tm_shape(cip_smoothRast$iso) +
    	tm_lines(col = "level",palette=topo.colors(7),
    	         legend.col.show = FALSE) + 
    tm_shape(cipSF %>% mutate(Award = Award / 1000000)) +  
    tm_symbols(size = .1, shape = 1, col = "Award",
               title.col = "Award in millions($)") + 
    	tm_legend( position=c("left", "top"), bg.color="gray", frame = FALSE) + 
    tm_layout(bg.color = "black")
	
```

Let's interact with our new raster with leaflet.

```{r}
m <- leaflet() %>% 
    setView(lng = -73.9691305, lat = 40.7764627, zoom = 11) %>% 
    addProviderTiles(providers$CartoDB.Positron)

kernelMask <- mask(kernelRast, nybb)
pal <- colorNumeric(c("#0C2C84", "#41B6C4", "#FFFFCC"), values(kernelMask),
  na.color = "transparent")

m %>% 
  addRasterImage(kernelMask, opacity = 0.8, color = pal,
group = "Kernel") %>%
  addLegend(pal = pal, values = values(kernelMask),
    title = "CIP Kernel Density", position = "topleft") %>% 
    addCircleMarkers(data = cip,
    stroke = FALSE, fillOpacity = 0.5,
    radius = 3, color = "purple",
    popup = ~`Building ID`,
        group = "Schools"
  ) %>% 
    addLayersControl(
    overlayGroups = c("Kernel", "Schools"),
    options = layersControlOptions(collapsed = FALSE)
  )
```


## Distance Based Analysis

We just looked at density based analysis, but what about on distance? We should also be interested in 
the distance based relationship between points also. 

### Average Nearest Neighbor

An average nearest neighbor (ANN) analysis measures the average distance from each point in the study area to its nearest point. 

The parameter k can take on any order neighbor (up to n-1 where n is the total number of points).

```{r}
# average distance to first nearest neighbor
annCIP <- mean(nndist(cipPPP, k=1))
annCIP

# average distance to second nearest neighbor
mean(nndist(cipPPP, k=2))
```

The average nearest neighbor function can be expanded to generate an ANN vs neighbor order plot. In the following example, we’ll plot ANN as a function of neighbor order for the for all of the neighbors:

```{r}
ANN <- 1:(cipPPP$n-1) %>% 
    as.list() %>% 
    purrr::map_dbl(function(x)mean(nndist(cipPPP, k = x)))

plot(ANN ~eval(1:(cipPPP$n-1)), type = "b",main = "Average Nearest Neighbor")
```

The bottom axis shows the neighbor order number and the left axis shows the average nearest neighbor distance in feet.

Here are two different views of the cluster and their average nearest neighbor plot:

```{r ANNcluster, echo=FALSE, fig.cap="Three different point patterns: a single cluster, a dual cluster and a randomly scattered pattern."}
knitr::include_graphics('https://mgimond.github.io/Spatial/11-Point-Patterns_files/figure-html/f11-diff-patterns-1.png')
```

```{r ANNcluster2, echo=FALSE, fig.cap="Three different ANN vs. neighbor order plots. The black ANN line is for the first point pattern (single cluster); the blue line is for the second point pattern (double cluster) and the red line is for the third point pattern."}
knitr::include_graphics('https://mgimond.github.io/Spatial/11-Point-Patterns_files/figure-html/f11-diff-ANN-plots-1.png')
```

Popular spatial analysis techniques compare observed point patterns to ones generated by an 
independent random process (IRP) also called complete spatial randomness (CSR). 
Where any event has equal probability of being in any location at a given time.

Average Nearest Neighbors Ratio:

$$ANN_{ratio}=\dfrac{ANN}{ANN_{expected}}$$

Average Nearest Neighbor Expected:

$$ANN_{Expected}=\dfrac{0.5}{\sqrt{n/A}} $$

Remember the average nearest neighbor for schools is 1511 feet

```{r}
annCIP
```

our ratio is 0.5 which means that our locations are randomly distributed. 
```{r}
annRatio <- annCIP / (0.5/(sqrt(cipPPP$n/area(cipPPP))))
annRatio
```

Do you see a problem here? Could different shapes encompassing the same point pattern have the same surface area? 
If so, shouldn't’t the shape of our study area be a parameter in our ANN analysis? 

```{r minBoundingGeo, echo=FALSE, fig.cap="Different conceptualizations of the minimmum bouding geometry"}
knitr::include_graphics('https://pro.arcgis.com/en/pro-app/tool-reference/data-management/GUID-EFBA5DB8-A161-4FE1-B2CB-792B65DC1469-web.png')
```

We can see how if we change our area of study the ratio will go up and 
a dispersed pattern can be observed. 

```{r}
annRatio <- annCIP / (0.5/(sqrt(cipPPP$n/(area(cipPPP)/10))))
annRatio
```

Now we'll create the simulation using Monte Carlo Methods to compare a randomly
distributed average nearest neighbor scenario.


```{r}
# Number of simulations
n     <- 599L       
# Create an empty object to be used to store simulated ANN values
ann.r <- vector(length = n) 

for (i in 1:n){
    # Generate random point locations in NYC
    rand.p   <- rpoint(n=cipPPP$n, win= as(nybb,"Spatial") %>% as("owin"))
    # Tally the ANN values
    ann.r[i] <- mean(nndist(rand.p, k=1)) 
}
```

let's take a look at what random points distributed along NYC would look like:

```{r}
Window(rand.p) <- as(nybb,"Spatial") %>% as("owin")
plot(rand.p, pch=16, main=NULL, cols=rgb(0,0,0,0.5))
```

Can you easily tell that it's random? How sure can you be? more on that later...

Let's see the distribution of the ANNs from the Monte Carlo simulation.

```{r}
tibble(ANN = ann.r) %>% ggplot(aes(x = ANN)) + 
    geom_histogram(fill = "green", col = "gray") +theme_minimal()
```

... and where does our actual ANN value sit?

```{r}

tibble(ANN = ann.r) %>% ggplot(aes(x = ANN)) + 
    geom_histogram(fill = "green", col = "gray") +
    geom_vline(xintercept = annCIP,linetype = "dashed") + theme_minimal()
```

A (pseudo) p-value can be extracted from a Monte Carlo simulation. We’ll work off of the last simulation. First, we need to find the number of simulated ANN values greater than our observed ANN value.

```{r}
N.greater <- sum(ann.r > annCIP)
p <- min(N.greater + 1, n + 1 - N.greater) / (n +1)
p
```

In our working example, you’ll note that or simulated ANN value was nowhere near the range of ANN values computed under the null yet we don’t have a p-value of zero. This is by design since the strength of our estimated p will be proportional to the number of simulations–this reflects the chance that given an infinite number of simulations at least one realization of a point pattern could produce an ANN value more extreme than ours.


It’s important to remember that the ANN tool is a distance based approach to point pattern analysis. Even though we are randomly generating points following some underlying probability distribution map we are still concerning ourselves with the repulsive/attractive forces that might dictate the placement of locations relative to one another–i.e. we are not addressing the question “can some underlying process explain the X and Y placement of the stores”.

# Global Methods

## Spatial Autocorrelation

So if you are familiar with timeseries forecasting, autocorrelation should ring a bell.
autocorrelation is when a variable is correlated with itself and this could happen over time
or... space. This is the crux of Tobler's first law of geography "everything is related 
to everything else, but near things are more related than distant things." 

Classic statistics assume that variables are independent but in space we observe that
near things are more related. 

Clusters are examples of positive spatial correlations while a checker board pattern
is a negative correlation. This is very similar to normal correlations when you think 
about the directionality of the relationship between two variables. No correlation
in our case means randomness. 

```{r spatialautocorrelation, echo=FALSE}
knitr::include_graphics('https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/GUID-5CCEE7E5-839C-46E8-A88B-FCD02F07B209-web.gif')
```

Here's a great example of population density and elevation. We know intuitively
that people usually live in clusters and that elevation is closely
dependent on it's neighbors (except for cliffs!).

```{r autocorrelation, echo=FALSE}
knitr::include_graphics('https://mgimond.github.io/Spatial/img/Random_maps.png')
```

Let's read in some NYC census tract data and it's shapefile.

```{r}
nyct <- read_sf(file.path(dataDir, "gis/nyct2010"))
nycACS <- read_csv(file.path(dataDir, "nycACS2017.csv"))
nycACS %<>% dplyr::select(Geo_COUNTY, Geo_TRACT, SE_A14006_001) %>% 
    dplyr::rename(BoroCode = Geo_COUNTY,
                  CT2010 =Geo_TRACT,
           MedIncome = SE_A14006_001) %>% 
    dplyr::mutate(BoroCode = case_when(
        BoroCode == "047" ~"3",
        BoroCode == "061" ~"1",
        BoroCode == "081" ~"4",
        BoroCode == "085" ~"5",
        BoroCode == "005" ~ "2"
        
    ))
st_crs(nyct)
nyct = st_set_crs(nyct, 2263)

nyct <- nyct %>% left_join(nycACS, by = c("BoroCode" = "BoroCode", 
                                          "CT2010" = "CT2010")) %>% 
    mutate(MedIncome = tidyr::replace_na(MedIncome, 0 ))
```

and plot out median income. It looks like there might be clusters of
similar income groups, but we need to say if it's happening
more than we expect if things were random.

```{r}
tm_shape(nyct) + tm_polygons(col = "MedIncome")
```

### Neighbors

Defining Neighboring polygons can take many forms. The common ones come from
chess, where it could be shared edges, corners, or both. There are also distance
based measures of neighbors that we'll explore a little later. We have also
already since $k$ nearest neighbors which is an extension of distance based 
neighbors but up to $k$ of them.

```{r autocorrelation2, echo=FALSE}
knitr::include_graphics('http://www.lpc.uottawa.ca/publications/moransi/image2.gif')
```

for creating neighbors from sf objects check out this [article](https://cran.r-project.org/web/packages/spdep/vignettes/nb_sf.html)

we are going to use the `poly2nb` function with the queen case as `TRUE` to 
create our neighbors list. 

```{r}
library(spdep)
nyct <- nyct %>% 
    mutate(MedIncome = if_else(MedIncome == 0, 0.1, MedIncome)) # adding a little noise for 0s

nyctSP  <- as(nyct, "Spatial")


nb <- poly2nb(nyctSP, queen=TRUE)

nb

# nb <- subset(nb,
#   !(1:length(nb) %in% c(612, 806, 1623)))
nb[[1]]
```

inspecting our neighbors list, we can see that three polygons
do not have any neighbors! This is not good and will have to be removed. 

```{r}
tm_shape(nyct) + tm_borders(col = "gray") + 
tm_shape(nyct[c(612, 806, 1623),]) + tm_fill(col = "red") + 
    tm_text(text = "CT2010", size = .6, remove.overlap = FALSE, col = "black")
```

You can also see when we call the first item in our neighbors list, it returns
the ids for it's neighbors which matches the queens case!

Now we can also map it back to our census tracts and pinpoint our census tracts
and which ones are the ones that are neighbors

```{r}
nyctSP$CT2010[1]
nyctSP$CT2010[nb[[1]]]
```

You can see our first census tract is in red on Staten Island, and the 
neighbors are in orange.

```{r}
tm_shape(nyct[nb[[1]],]) + tm_fill(col = "orange") + 
    tm_shape(nyct[1,]) + tm_fill(col = "red") +
    tm_shape(nyct) + tm_borders(col = "gray")  + 
    tm_text( text = "BoroCT2010")
```

Our neighbors list is really just a network if you think about it. We can
visualize it similarly to network analysis graphs and draw edges between
our nodes, which is our census tract centroids. 

```{r}
par(bg = 'black')
plot(nb, coordinates(nyctSP), points=FALSE, lwd = 0.5, col = "#DC16C3")
```

Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight `(style="W")`. This is accomplished by assigning the fraction $1/(\text{# of neighbors})$to each neighboring county then summing the weighted income values. While this is the most intuitive way to summaries the neighbors’ values it has one drawback in that polygons along the edges of the study area will base their lagged values on fewer polygons thus potentially over- or under-estimating the true nature of the spatial autocorrelation in the data. For this example, we’ll stick with the `style="W"` option for simplicity’s sake but note that other more robust options are available, notably `style="B"`.

```{r}
nb2 <- poly2nb(nyctSP[-c(612,806,1623),], queen=TRUE)

lw <- nb2listw(nb2, style="W", zero.policy=TRUE)
```
To see the weight of the first polygon’s four neighbors type:
```{r}
lw$weights[[1]]
```
each neighbor gets a fifth of the weight.


Finally, we’ll compute the average neighbor income value for each polygon. These values are often referred to as spatially lagged values.
Again if you think about timeseries forecasting. If you have worked with auto-regressive models you can
think about the ACF plot which returns the autocorrelation of a series with it's lagged values.
It shows us how the present value is related with its past values and if there is
seasonality(whether it is weekly, monthly, quarterly, etc...) we'll see a high
autocorrelation value at those lag points!

```{r autoregression, echo=FALSE, fig.cap="Sine wave(left) and the coorelogram of its ACF(right)."}
knitr::include_graphics("https://nwfsc-timeseries.github.io/atsa-labs/Applied_Time_Series_Analysis_files/figure-html/tslab-plotSineACF-1.png")
```

```{r}
nyctMedInc <-  nyct %>% 
    slice(-c(612, 806, 1623)) %>% 
    pull(MedIncome)

MedIncLag <- lag.listw(lw, nyctMedInc)
```

Now that we have gotten through the concepts of what criteria a neighbor consists of
and creating those lagged variables. If we plot the lagged median income(mean income
of neighbors) vs. the median income and get the OLS regression line, that is the 
Moran's I which is an measure of spatial autocorrelation. [Geary's C](https://en.wikipedia.org/wiki/Geary%27s_C)
is another often used statistic, but we'll dive into Moran's I further in the next section.

```{r laggedRegression, echo=FALSE, fig.cap="The lagged variable plotted against the variable"}
knitr::include_graphics("https://mgimond.github.io/Spatial/img/MoranI_scatter_plot.png")
```

## Moran's I

Moran's I is a common measure of spatial autocorrelation, which is an extension of temporal autocorrelation from time series analysis. While time is only one dimensional, in spatial statistics, we are dealing with two dimensions(usually, could be more). Building on what we have learned in class already, it's up to the analyst to understand what "near" or neighbors mean in our analysis.

In normal autocorrelation, things like your weight change gradually over time so weight is highly correlated with the days that are consecutive. In spatial autocorrelation we try to see how a location, whether it is a polygon, point, or raster grid are similar to each other. 

The Spatial Autocorrelation (Global Moran's I) tool is an inferential statistic, which means that the results of the analysis are always interpreted within the context of its null hypothesis. For the Global Moran's I statistic, the null hypothesis states that the attribute being analyzed is randomly distributed among the features in your study area; said another way, the spatial processes promoting the observed pattern of values is random chance. Imagine that you could pick up the values for the attribute you are analyzing and throw them down onto your features, letting each value fall where it may. This process (picking up and throwing down the values) is an example of a random chance spatial process.

Here is the equation for Moran's I:

$$I = \frac N W \frac {\sum_i \sum_j w_{ij}(x_i-\bar x) (x_j-\bar x)} {\sum_i (x_i-\bar x)^2} $$
where $N$ is the number of spatial units indexed by $i$ and $ j$;$x$ is the variable of interest; $\bar{x}$ is the mean of $x$;$w_{ij}$ is a matrix of spatial weights with zeroes on the diagonal (i.e., $w_{ii}=0$); and $W$ is the sum of all $w_{ij}$.^[[Moran's I](https://en.wikipedia.org/wiki/Moran%27s_I)]

The values of $I$ usually range from -1 to 1. This shouldn't come as a surprise if you think of values that get outputted when we talk about correlations between two variables, there is usually a negative correlation (-1), no correlation (0), or positive correlation (1). In Moran's I -1 means evenly dispersed, 0 is random, and 1 is perfectly clustered. 

Let's take our lagged median income and make our plot comparing the median income
variable. We can see that our Moran's I value is 0.58 which is the correlation
coefficient between our lagged and non-lagged variable. 

```{r}
lagModel <- lm(MedIncLag ~ nyctMedInc)

data.frame(MedIncLag = MedIncLag,
           nyctMedInc = nyctMedInc) %>% 
    ggplot(aes(x= nyctMedInc, y = MedIncLag)) + 
    geom_point() + 
    geom_smooth(method = "lm", ) + 
    xlab("Median Income") + 
    ylab("Mean Neighbor Median Income") + 
    annotate("text", x = 45000, y = 200000, label = 
                 paste("Moran's I is", coef(lagModel)[2] %>% 
                          round(2) %>% 
                          as.character())) + 
    theme_minimal()
```

You can see where neighbors might have some effect on your census tract's median
household income.

```{r}
tm_shape(nyct %>% 
    slice(-c(612, 806, 1623)) %>% 
        mutate(MedIncLag = MedIncLag)) + 
    tm_polygons(col = "MedIncLag", n = 5, style = "jenks",
                title = "Lagged Median Income")
```

We can see that the tracts with larges negative differences are
parks, cemeteries, or JFK airport.

```{r}
tm_shape(nyct %>% 
    slice(-c(612, 806, 1623)) %>% 
        mutate(MedIncDiff = nyctMedInc - MedIncLag)) + 
    tm_polygons(col = "MedIncDiff", title = "Median Income Difference",
                palette = "RdBu", n = 7, midpoint = 0,
                style = "jenks")
```

There is also a premade function called `moran.test` that will take in a
numeric vector in our case the median income values from the census tracts
and the `listw` object which is our lagged value weights we specified
which in our case we distributed the weights equally among our neighbors.

```{r}
moran.test(nyctMedInc,lw)
```

The previous method was calculated analytically, but we could also calculate
our Moran's I through Monte Carlo simulation with the `moran.mc` function.
We feed in the same inputs but this also input how many times we want to run the 
simulation.

In the Monte Carlo method, the median income values are randomly assigned to the
census tracts and a moran's I value is computed for each simulation. We can then compute
the moran's I values if we randomly placed those original values compared to
our actual Moran's I.

```{r}
MC <- moran.mc(nyctMedInc, lw, nsim = 600)
MC
```

You can see that our Moran's I value is not very different between the two methods.

We can also plot the simulation output and see where our Moran's I score falls 
along the density plot of our correlation coefficients we would see if our
median income values was randomized.

```{r}
par(bg = "white")
plot(MC, main="", las=1)
```

## Moran's I as a function of distance

So remember when I said that defining a what  neighbor is can be subjective and
we really only talked about topological neighbors so far? We can also think of 
neighbors as a function of distance. We'll explore neighbors as the distance between
two centroids to determine closeness. 

First we will have to create coordinates with the `coordinates` function to convert
our `sp` object.

```{r}
nyctCoord <- coordinates(nyctSP)
```

now we will use the `dnearneigh` function to specify a lower distance band, upper distance band
to return our `nb` object much like earlier when we selected `queen = TRUE` with
`poly2nb`.

```{r}
dist1  <-  dnearneigh(nyctCoord, 0, 5280)  
dist1
noLinkd1 <- c(612, 806, 1094, 1382, 1383, 1623, 1667, 1677, 1712, 1724, 1760, 1891, 1941, 1942, 1950, 2095)
```

We will re run our function with our nolink coordinates.

```{r}
dist1_1  <-  dnearneigh(nyctCoord[-noLinkd1,], 0, 5280)  
dist1_1
```

We will now take our `nb` object `dist1_1` and create the `lw` object. We will use
Monte Carlo simulation again to determine the Moran's I number of our new
neighbor search results.

```{r}
lwD1 <- nb2listw(dist1_1, style="W",zero.policy=T) 
MI1  <-  moran.mc(nyct %>% slice(-noLinkd1) %>% pull(MedIncome), 
                  lwD1, nsim = 600, zero.policy=T) 
MI1
```

Our Moran's I value dropped a little when changed our search distance to a mile
or 5280 feet. Though compared to our simulations we are well above what you would
normally expect from random simulations.

```{r}
plot(MI1, main="", las=1) 
```

Now we will go to the extreme and show you what would happen if we select a lot more
neighbors with a search radius of 50,000 ft which is ~9.5 miles.

```{r}
dist2  <-  dnearneigh(nyctCoord, 0, 50000)  
dist2
```

You can see that our Moran's I has dropped close to 0. Even here though we can
see that our values are greater than what you would expect if it was simulated
and randomly assigned to polygons.

```{r}
lwD2 <- nb2listw(dist2, style="W",zero.policy=T) 
MI2  <-  moran.mc(nyct %>% pull(MedIncome), lwD2, 
                  nsim = 600, zero.policy=T) 
MI2
```

```{r}
plot(MI2, main="", las=1) 
```

```{r}
MedIncLag2 <- lag.listw(lwD2, nyct %>% pull(MedIncome))

lagModel2 <- lm(MedIncLag2 ~ nyct %>% pull(MedIncome))

data.frame(MedIncLag = MedIncLag2,
           nyctMedInc = nyct %>% pull(MedIncome)) %>% 
    ggplot(aes(x= nyctMedInc, y = MedIncLag)) + 
    geom_point() + 
    geom_smooth(method = "lm", ) + 
    xlab("Median Income") + 
    ylab("Mean Neighbor Median Income") + 
    annotate("text", x = 45000, y = 70000, label = 
                 paste("Moran's I is", coef(lagModel2)[2] %>% 
                          round(4) %>% 
                          as.character())) + 
    theme_minimal()
```

```{r}
tm_shape(nyct %>% mutate(MedIncLag2 = MedIncLag2)) + 
    tm_polygons(col = "MedIncLag2", n = 5, style = "jenks",
                legend.title = "Lagged Median Income")
```


# Local Statistics

So far we have been looking at the global scope or one number to determine
how much clustering/dispersion of the whole data set. Now we will look
at local clusters with the Local Moran's I. 

## Local Moran's I

This method calculates the local Moran's I value,
z-score, a pseudo p-value, and a code representing the cluster typr for each statistically 
significant feature. This will help us identify spatial clusters with high/low values
and even spatial outliers. 

A positive local Moran's I value indicates that feature has neighboring features with similarly
high or low values, while a negative values indicates that neighboring features
are dissimilar(an outlier). The p-value will allow us to determine if these values 
are statistically significant enough. 

```{r}
localMoran <- localmoran(nyctMedInc, lw)
nyctLocal <- nyct %>% slice(-c(612, 806, 1623)) %>% cbind(localMoran)
```

The outputs are:
- Ii: local moran statistic

- E.Ii: expectation of local moran statistic

- Var.Ii: variance of local moran statistic

- Z.Ii: standard deviate of local moran statistic

- Pr(): p-value of local moran statistic

```{r}
tm_shape(nyctLocal) + 
    tm_polygons(col = "Ii", lwd = 0.1, 
                palette = "-RdBu", n = 7, 
                midpoint = 0, style = "jenks") 
```

Remember we don't know which are high clusters, or low clusters. We just know
where are clusters and outliers, hot-spot analysis is another method.

## Getis-Ord General G

Also referred to as Local G is a high/low clustering technique. The resulting
z-score will tell you if there are high value clusters or low value clusters. 

The higher the z-score, the more intense the clustering of high values(hot spot)
are being exhibited in your map. On the converse the greater the negative z-scores
are, the more intense the clustering of low values(cold spot) is being detected.

```{r localG, echo=FALSE, fig.cap=""}
knitr::include_graphics("https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/GUID-3275CB40-D309-4326-818D-CF22E69B77B2-web.png")
```

(1) You can see that the only difference between the numerator and the denominator is $w_{i,j}$
which is the spatial weight. Also this method will only work with positive values and say 
you are working with binary outcomes, or positive values less than 1,then the 
resulting General G will also be between 0 and 1. 

```{r}
getisG <- localG(nyctMedInc, lw)
nyctLocal <- nyctLocal %>% mutate(localG = as.numeric(getisG))
```

In our map, we can see that the Upper West side, Upper East side, Midtown, most of Downtown,
downtown Brooklyn are all high median income clusters. Over in the Bronx most of them are 
low median income clusters as many parts of Brooklyn and Queens. 

```{r}
zbreaksG <- c(min(nyctLocal$localG), -2.58, -1.96, -1.65, 1.65, 1.96, 2.58, max(nyctLocal$localG))
```

```{r}
tm_shape(nyctLocal) + 
    tm_polygons(col = "localG", lwd = 0.1, palette = "-RdBu",
                breaks = zbreaksG) 
```

With natural Jenks breaks for classification.

```{r}
tm_shape(nyctLocal) + 
 tm_polygons(col = "localG", lwd = 0.1, palette = "-RdBu",
             n = 7, midpoint = 0, style = "jenks")
```


## Getis Gi*

The Getis Gi* (pronounced "G-i-star") is also called hot spot analysis. The resulting
z-score will tell you if there are high value clusters or low value clusters. 

The higher the z-score, the more intense the clustering of high values(hot spot)
are being exhibited in your map. On the converse the greater the negative z-scores
are, the more intense the clustering of low values(cold spot) is being detected.

```{r localGstar, echo=FALSE, fig.cap=""}
knitr::include_graphics("http://resources.esri.com/help/9.3/arcgisengine/java/gp_toolref/spatial_statistics_tools/Getis-Ord%20Local%20Statistics.png")
```

To get our getis gi* score we will use the `localG` function. One important note
is that your `lw` object needs to have the `self.included` attribute
set to `TRUE` to get the feature being analyzed included. This is the main difference
between the Gi* and other local spatial statistics like local Moran's I. 

```{r}
nb2Self <- include.self(nb2)
lwSelf <- nb2listw(nb2Self, style="W", zero.policy=TRUE)
getisGstar <- localG(nyctMedInc, lwSelf)
nyctLocal <- nyctLocal %>% mutate(Gistar = as.numeric(getisGstar))
```

In our map, we can see that the Upper West side, Upper East side, Midtown, most of Downtown,
downtown Brooklyn are all high median income clusters. Over in the Bronx most of them are 
low median income clusters as many parts of Brooklyn and Queens. 

```{r}
zbreaksGstar <- c(min(nyctLocal$Gistar), -2.58, -1.96, -1.65, 1.65, 1.96, 2.58, max(nyctLocal$Gistar))
```

```{r}
tm_shape(nyctLocal) + 
    tm_polygons(col = "Gistar", lwd = 0.1, palette = "-RdBu",
                breaks = zbreaksGstar) 
```

With natural Jenks breaks for classification.

```{r}
tm_shape(nyctLocal) + 
 tm_polygons(col = "Gistar", lwd = 0.1, palette = "-RdBu",
             n = 7, midpoint = 0, style = "jenks")
```

Here's a quick example of reading results from this method:

So, when doing a hot spot analysis using Getis-Ord Gi*, if you have chosen a scale for your analysis that is small, even though there are features around with low values, it is definitely conceivable that a feature with a very high value would show up as a hot spot even though it is surrounded by low values (essentially the value of the feature is so high that it brings the local mean up).  Alternatively, it makes sense that the same feature, when analyzed using Local Moran's I, would show up as a High surrounded by Low values.  Essentially, both analyzes are right, it just depends on the question that you're asking which result makes sense for you.
[link](https://community.esri.com/message/410840?commentID=410840#comment-410840)

# Spatial Lag

spatial lag is a smoother, which is created by the weighted average of neighboring 
values. more similar to distributed lag rather than temporal lag. If there are four
neighbor the weight is 1/4 which is a convention to split weights equally. 

This way you can bring in spatial variables as new explanatory variables, or 
dependent variables. 

You can now include spatially lagged variables into regression

- Spatial Lag Model: spatially lagged dependent variable is used
- Spatial Error Model: spatially lagged error term is used
- Spatial Cross-Regressive Model: spatially lagged explanatory variables are added

## Inverse Distance Weighted

Inverse distance weighted (IDW) interpolation determines cell values using a linearly 
weighted combination of a set of sample points. The weight is a function of inverse distance. 
The surface being interpolated should be that of a spatially dependent variable.

```{r}
elevation <- read_csv(file.path(dataDir, "elevation.csv"))

elevationSF <-  st_as_sf(elevation,coords = c("Lon", "Lat"),crs = 4326)
elevationSF <- st_transform(elevationSF, crs = 2263)

elevationsp <- as(elevationSF, "Spatial")
tm_shape(nybb) + tm_polygons() + 
tm_shape(elevationSF) + tm_symbols(size= .1, shape = 1, col = "green", alpha = .3) 
# Create an empty grid where n is the total number of cells
grd              <- as.data.frame(spsample(elevationsp, "regular", n=50000))
names(grd)       <- c("X", "Y")
coordinates(grd) <- c("X", "Y")
gridded(grd)     <- TRUE  # Create SpatialPixel object
fullgrid(grd)    <- TRUE  # Create SpatialGrid object
# Add P's projection information to the empty grid
proj4string(grd) <- proj4string(elevationsp)
# Interpolate the grid cells using a power value of 2 (idp=2.0)
P.idw <- gstat::idw(ELEVATION ~ 1, elevationsp, newdata=grd, idp=2)

# Convert to raster object then clip to Texas
r       <- raster(P.idw)
r.m     <- mask(r, nybb)
# Plot
tm_shape(r.m) + 
  tm_raster(style = "cont", palette = "Greens",
            title="Elevation \n(in feet)") + 
  tm_shape(cipSF) + tm_symbols(size=0.1, shape = 1) 
```

```{r, results="hide"}
# Leave-one-out validation routine
IDW.out <- vector(length = length(elevationsp))
for (i in 1:length(elevationsp)) {
  IDW.out[i] <- gstat::idw(ELEVATION ~ 1, elevationsp[-i,], elevationsp[i,], idp=2)$var1.pred
}

```

```{r}
# Plot the differences
OP <- par(pty="s", mar=c(4,3,0,0))
  plot(IDW.out ~ elevationsp$ELEVATION, asp=1, xlab="Observed", ylab="Predicted", pch=16,
       col=rgb(0,0,0,0.5))
  abline(lm(IDW.out ~ elevationsp$ELEVATION), col="red", lw=2,lty=2)
  abline(0,1)
par(OP)
```

```{r}
# Compute RMSE
sqrt(sum((IDW.out - elevationsp$ELEVATION)^2) / length(elevationsp))
```

## Cross-validation

```{r, results="hide"}
# Implementation of a jackknife technique to estimate 
# a confidence interval at each unsampled point.

# Create the interpolated surface
img <- gstat::idw(ELEVATION~1, elevationsp, newdata=grd, idp=2.0)
n   <- length(elevationsp)
Zi  <- matrix(nrow = length(img$var1.pred), ncol = n)

# Remove a point then interpolate (do this n times for each point)
st <- stack()
for (i in 1:n){
  Z1 <- gstat::idw(ELEVATION~1, elevationsp[-i,], newdata=grd, idp=2.0)
  st <- addLayer(st,raster(Z1,layer=1))
  # Calculated pseudo-value Z at j
  Zi[,i] <- n * img$var1.pred - (n-1) * Z1$var1.pred
}


```

```{r}
# Jackknife estimator of parameter Z at location j
Zj <- as.matrix(apply(Zi, 1, sum, na.rm=T) / n )

# Compute (Zi* - Zj)^2
c1 <- apply(Zi,2,'-',Zj)            # Compute the difference
c1 <- apply(c1^2, 1, sum, na.rm=T ) # Sum the square of the difference

# Compute the confidence interval
CI <- sqrt( 1/(n*(n-1)) * c1)

# Create (CI / interpolated value) raster
img.sig   <- img
img.sig$v <- CI /img$var1.pred 

# Clip the confidence raster to Texas
r <- raster(img.sig, layer="v")
r.m <- mask(r, nybb)

# Plot the map
tm_shape(r.m) + tm_raster(n=7,title="95% confidence interval \n(in feet)") +
    tm_shape(elevationsp) + tm_symbols(size=0.1, shape = 1) 
```

# Regression Analysis

Normal Ordinary Least Squares regression (OLS) have some assumptions that spatial
data violates. A misspecified model is one that is not complete, it is where
you are missing important explanatory variables. This can be easily diagnosed
when you map the residuals and find significant spatial autocreelation. Say that
you find that the model is always over predicting in higher elevations and under predicting
in the lower elevations, then your model is probably missing an elevation variable.

There are times with the missing variable(s) are complex and or difficult to measure.
In such cases other spatial regression methods can be deployed such as Geographically
Weighted Regression.

**Regression Pitfalls**

- omitted explanatory variables
- nonlinear relationships
- outliers
- nonstationarity (regional variation)
- multicollinearity

**Regaionl variation**
ways to overcome spatial variation in your OLS regression:

1) include a variable in the model that explains the regional variation,
it could be zipcode, elevation, etc.
1) use methods like geographically weighted regression
1) Change the scale of the study area so that stationarity can be expected ie.
no regional variation is exhibited. 

## OLS Global Regression

Let's read in out nySales data, and clean the data a bit and get the condo sales
only.

```{r}
nySales <- read_csv(file.path(dataDir, "nycSales.csv"))
set.seed(1234)
condos <- nySales %>% filter(type == "condo") %>% 
    tidyr::separate(gps_coordinates, c("Lat", "Lon"), " ") %>%
    mutate(Lat = as.numeric(Lat),
           Lon = as.numeric(Lon)) %>% 
    filter(Lat > 0, borough != "staten_island") %>% 
    mutate(z = scale(price),
           zLat = scale(Lat)) %>% 
    filter(z < 3,(zLat < -4 | zLat < 4)) %>% 
    mutate(num_baths = as.numeric(num_baths),
           num_beds = as.numeric(num_beds),
           num_beds = if_else(is.na(num_beds), 0, num_beds),
           borough = as.factor(borough)) %>% 
    filter(complete.cases(.)) %>% 
    group_by(borough) %>% 
    sample_frac(0.14) %>% 
    ungroup()

```

Now we can look at the sale prices(logged) to see the 
distribution.

```{r}
condos %>% ggplot(aes(log(price))) + 
    geom_histogram(fill = "light blue") + 
    theme_minimal()
```

We can create a multiple regression with num_beds, num_baths,
num_sqft, built_date to predict our price.

```{r}
mod1 <- lm(log(price) ~  num_beds + num_baths + num_sqft + built_date, condos)
summary(mod1)
sqrt(mean(mod1$residuals^2))
```

We can see that we have some outliers in our first plot. We can also see that 
in our coefficient plot num_baths has a strong effect on the price,
followed by num_beds. In real estate development, the most expensive rooms to
build are kitchens and bathrooms which would validate these results.

```{r}
plot(mod1, which = 3)
coefplot::coefplot(mod1, sort = "mag", intercept = FALSE) + 
    theme_minimal()
```

Now let's get our residuals

```{r}
resids1 <- residuals(mod1)
condoSF <- condos %>% mutate(resids1 = resids1) %>% 
    st_as_sf(coords = c("Lon", "Lat"),crs = 4326) %>% 
    st_transform(crs = 2263)
```

by mapping out our residuals, we can definitely see that there are other factors
that our causing some autocorrelation. If you looks up at Harlem and in downtown.
```{r}
nycNeighb <- read_sf(file.path(dataDir, "gis/nynta_19a"))
nycNeighb <- st_transform(nycNeighb, crs = 2263)
```

```{r}
tm_shape(nycNeighb) + tm_borders() + 
    tm_shape(condoSF) + tm_dots(col ="resids1", palette = "RdBu", n = 7, midpoint = 0, style = "fisher",
                                   size =0.1, alpha = 0.5) 
```

What if we add the borough variable?

we can see that our RMSE score is now 0.44 compared to 0.51. We are doing a bit better
when we are trying to explain our condo sale prices.

```{r}
mod2 <- lm(log(price) ~  borough + num_beds + num_baths + num_sqft + built_date, condos)
summary(mod2)
sqrt(mean(mod2$residuals^2))
```

We'll also see that Manhattan, Brooklyn, and Queens have big coefficients.

```{r}
plot(mod2, which = 3)
coefplot::coefplot(mod2, sort = "mag", intercept = FALSE) + 
    theme_minimal()
```

Now let's get our residuals again

```{r}
resids2 <- residuals(mod2)
condoSF <- condoSF %>% mutate(resids2 = resids2) 
```

By mapping out our residuals, we'll see that we are doing a bit better,
but visually you can see that our residuals are still autocorrelated.
There must be other explanatory variables that we haven't captured yet.

```{r}
tm_shape(nycNeighb) + tm_borders() + 
    tm_shape(condoSF) + tm_dots(col ="resids2", palette = "RdBu", n = 7, midpoint = 0, style = "fisher",
                                   size =0.1, alpha = 0.5) 
```

We can look to see if our residuals are normally distributed around our OLS regression
line, but we can see that by mapping them out and with our new knowledge about
spatial autocorrelation, we can see that our residuals are not spatially distributed 
randomly.

## Geographically Weighted Regression

Global models, like OLS regression, create equations that best describe the overall data relationships in a study area. When those relationships are consistent across the study area, the OLS regression equation models those relationships well. When those relationships behave differently in different parts of the study area, however, the regression equation is more of an average of the mix of relationships present, and in the case where those relationships represent two extremes, the global average will not model either extreme well. When your explanatory variables exhibit nonstationary relationships (regional variation), global models tend to fall apart unless robust methods are used to compute regression results.

- Include a variable in the model that explains the regional variation. If you see that your model is always over predicting in the north and under predicting in the south, for example, add a regional variable set to 1 for northern features and set to 0 for southern features.
- Use methods that incorporate regional variation into the regression model such as geographically weighted regression.
- Redefine/Reduce the size of the study area so that the processes within it are all stationary (so they no longer exhibit regional variation).

Geographically Weighted Regression (GWR) is one of several spatial regression techniques increasingly used in geography and other disciplines. GWR provides a local model of the variable or process you are trying to understand/predict by fitting a regression equation to every feature in the data set. GWR constructs these separate equations by incorporating the dependent and explanatory variables of features falling within the bandwidth of each target feature. The shape and size of the bandwidth is dependent on user input for the Kernel type, Bandwidth method, Distance, and Number of neighbors parameters.

An examination of the spatial variability among the local regression coefficients can 
illuminate spatial heterogeneities ie local deviations from the global regression model.


take weighted average of $y$ observed for $x$ close to $x_0$
So it is basically a moving weighted average for every x point
resulting in a combination of weighted averages. 

local regression vs GWR is that geographic closeness vs variable closeness.  so large and it's one slope, too little and there's no
variation sample of one for selecting the right bandwidth. adaptive kernel for sparse or dense locations. used more as an exploratory
tool, we haven't explained anything yet only that the relationship we posited isn't stable across space. 


Let's convert our `condoSF` to a `sp` object

```{r}
condoSP <- as(condoSF, "Spatial")
```

first we will calibrate the bandwidth of the kernel that will be used to capture the points for each regression
(this will take a little while).

The bandwidth is hard to specify a priori and the preferred approach is
to carry out a cross-validation, minimizing the root mean square prediction
error.

Once the bandwidth has been found, or chosen by hand, the gwr function may be
used to fit the model with the chosen local kernel and bandwidth.

This function creates an object that contains a dataframe with the individual coefficient estimates
for each variable and each location. Also it can run in parallel so we can use the
`parallel` package to spin up a cluster.

We first will need to create `condoBW` to select a suitable number of neighbors.
You can use `gwr.sel` to either choose a bandwidth or number of neighbors. The default
is to use cross validation, but the Akaike Information Criterion could also be used.

```{r}
no_cores <- parallel::detectCores() - 1 # Calculate the number of cores
cl <-parallel:: makeCluster(no_cores)# Initiate cluster
condoBW <- gwr.sel(log(price) ~  num_beds + num_baths + num_sqft + built_date, 
                   data=condoSP,gweight = gwr.Gauss, verbose = FALSE, adapt  = TRUE)
condoGWR <-spgwr::gwr(log(price) ~  num_beds + num_baths + num_sqft + built_date, data=condoSP, adapt = condoBW,
                  gweight = gwr.Gauss, hatmatrix = TRUE, se.fit = TRUE, cl = cl)
parallel::stopCluster(cl)
```

we can checkout the results for our GWR. There are a lot of values and many of them can be 
found in this [link](http://desktop.arcgis.com/en/arcmap/10.3/tools/spatial-statistics-toolbox/interpreting-gwr-results.htm)
but the residual sum of squares, quasi-global R2and the statistics on the explanatory
coefficients can be found.

```{r}
condoGWR
```

We can also calculate the RMSE score which is  0.42, which is slightly 
better than our OLS models. 

```{r}
sqrt(mean(condoGWR$SDF$gwr.e^2))
```

let's save our results from our gwr as a data.frame and add it as columns
into our `sf` object from before so we can start exploring it.
```{r}
results <- as.data.frame(condoGWR$SDF)
results %>% head()
condoComb <- condoSF %>% 
    mutate(
           coefnum_beds = results$num_beds,
           coefnum_baths = results$num_baths,
           coefbuilt_date = results$built_date,
           coefnum_sqft = results$num_sqft,
           residuals = results$gwr.e)
```

Let's look at the regional coefficients we got for the number of baths. You can see
that there are areas where the coefficient for a number of bath is much greater
than other parts of NYC

```{r}
tm_shape(condoComb %>% filter(!is.na(coefnum_baths))) + tm_dots(col ="coefnum_baths", 
                                   palette = "RdBu", n = 7, midpoint = 0,
                                   size =0.1, title = "Num Bath Coefficient") + 
    tm_shape(nycNeighb) + tm_borders() + 
    tm_legend(position = c("left", "top"))
```

For our build date, we cans ee that some places have a negative effect on condo
prices while some areas have a positive effect. Here is an instance where maybe
squaring older buildings and creating a parabolic curve for built date might be
a helpful transformation.

```{r}
tm_shape(condoComb %>% filter(!is.na(coefbuilt_date))) + tm_dots(col ="coefbuilt_date", 
                                  palette = "RdBu", n = 7, midpoint = 0,
                                   size =0.1, title = "Built Date Coefficient") + 
    tm_shape(nycNeighb) + tm_borders() + 
    tm_legend(position = c("left", "top"))
```

We can also see for our number of beds, the pattern of coefficients are similar 
to that of the number of beds, which could mean that there is multicollinearity.

```{r}
tm_shape(condoComb %>% filter(!is.na(coefnum_beds))) + tm_dots(col ="coefnum_beds", 
                                   palette = "RdBu", n = 7, midpoint = 0,
                                   size =0.1, title = "Num Beds Coefficient") + 
    tm_shape(nycNeighb) + tm_borders() + 
    tm_legend(position = c("left", "top"))
```

we can again map out our residuals and see that visually it looks like we still
have autocorrelation, but it seems to be less prominent than before.

```{r}
tm_shape(condoComb %>% filter(!is.na(residuals))) + tm_dots(col ="residuals", 
                                  palette = "RdBu", n = 7, midpoint  = 0,
                                   size =0.1, title = "GWR Residuals") + 
    tm_shape(nycNeighb) + tm_borders()
```

We can see what our average nearest neighbor distance is for our condos for our
neighbor ratio we got from our gwr which was 0.14. this makes our number of neighbors
out of ~2,550 that are neighbors as ~350 closest condos. 

The mean distance is 8,291 ft and the median distance is 6,500 feet.

```{r}
condoPPP <- condoComb %>% as("Spatial") %>% as("ppp")
mean(nndist(condoPPP, k = 350))
median(nndist(condoPPP, k = 350))
```

For simplicity sake I'm going to use 1.25 miles or 6,600 feet for now to go through
our moran's I calculation which is close to our median average 350th nearest neighbor.

```{r}
condoDist  <-  dnearneigh(condoComb, 0, 6600) 
condoDist
noLinkCondo <- c(244, 2492, 2518, 2542)
condoDist2  <-  dnearneigh(condoComb  %>% "["(-noLinkCondo,), 
                           0, 6600)  
condoDist2
```

we can plot out what our edges look like in our `nb` object.
it will look pretty dense since most condos should have 350 edges
created to other condos.

```{r}
par(bg = 'black')

plot(condoDist2, st_coordinates(condoComb %>% 
                                "["(-noLinkCondo,)), 
     points=FALSE, lwd = 0.5, col = alpha("#DC16C3", 0.05))
```

Now you will see that our Moran's I value is getting close to 0.
This means that we are getting closer to spatial randomness in our residuals
which is a good thing.

```{r}
lw3 <- nb2listw(condoDist2, style="W", zero.policy=TRUE)
MI3  <-  moran.mc(condoComb %>% 
                      "["(-noLinkCondo,) %>% 
                      pull(residuals), 
                  lw3, nsim=600,zero.policy=T) 

MI3
```
